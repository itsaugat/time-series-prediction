{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import LSTM, Bidirectional, Dense, Dropout, RepeatVector, TimeDistributed, Lambda, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "#from tcn import TCN\n",
    "from nbeats_keras.model import NBeatsNet\n",
    "#from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/EURGBP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>0.6547</td>\n",
       "      <td>0.6547</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>0.6504</td>\n",
       "      <td>0.6504</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>0.6503</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>0.6475</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>0.6496</td>\n",
       "      <td>0.6516</td>\n",
       "      <td>0.6466</td>\n",
       "      <td>0.6472</td>\n",
       "      <td>0.6472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-07</td>\n",
       "      <td>0.6498</td>\n",
       "      <td>0.6510</td>\n",
       "      <td>0.6477</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close  Adj Close  Volume\n",
       "0  2003-01-01  0.6547  0.6547  0.6497  0.6504     0.6504     0.0\n",
       "1  2003-01-02  0.6503  0.6627  0.6475  0.6494     0.6494     0.0\n",
       "2  2003-01-03  0.6496  0.6516  0.6466  0.6472     0.6472     0.0\n",
       "3  2003-01-06  0.6471  0.6523  0.6467  0.6502     0.6502     0.0\n",
       "4  2003-01-07  0.6498  0.6510  0.6477  0.6490     0.6490     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>0.85700</td>\n",
       "      <td>0.86070</td>\n",
       "      <td>0.84913</td>\n",
       "      <td>0.85700</td>\n",
       "      <td>0.85700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>0.85520</td>\n",
       "      <td>0.85550</td>\n",
       "      <td>0.85290</td>\n",
       "      <td>0.85530</td>\n",
       "      <td>0.85530</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>0.85370</td>\n",
       "      <td>0.85632</td>\n",
       "      <td>0.85095</td>\n",
       "      <td>0.85380</td>\n",
       "      <td>0.85380</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0.85406</td>\n",
       "      <td>0.85558</td>\n",
       "      <td>0.85128</td>\n",
       "      <td>0.85406</td>\n",
       "      <td>0.85406</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.85419</td>\n",
       "      <td>0.85480</td>\n",
       "      <td>0.84544</td>\n",
       "      <td>0.85426</td>\n",
       "      <td>0.85426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     Open     High      Low    Close  Adj Close  Volume\n",
       "4430  2019-12-25  0.85700  0.86070  0.84913  0.85700    0.85700     0.0\n",
       "4431  2019-12-26  0.85520  0.85550  0.85290  0.85530    0.85530     0.0\n",
       "4432  2019-12-27  0.85370  0.85632  0.85095  0.85380    0.85380     0.0\n",
       "4433  2019-12-30  0.85406  0.85558  0.85128  0.85406    0.85406     0.0\n",
       "4434  2019-12-31  0.85419  0.85480  0.84544  0.85426    0.85426     0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.1\n",
    "valid_frac = test_frac\n",
    "split = len(data) - round((test_frac+valid_frac)*len(data))\n",
    "#print(split)\n",
    "val = round(valid_frac*len(data))\n",
    "#print(val)\n",
    "df_train = data[:split]\n",
    "df_valid = data[split:split+val]\n",
    "df_test = data[split+val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3525\n",
      "441\n",
      "440\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_valid))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#sc = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "df_train_scale = sc.fit_transform(df_train['Close'].values.reshape(-1,1))\n",
    "df_valid_scale = sc.transform(df_valid['Close'].values.reshape(-1,1))\n",
    "df_test_scale = sc.transform(df_test['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51852694],\n",
       "       [-1.53103984],\n",
       "       [-1.55856822],\n",
       "       ...,\n",
       "       [ 1.00107028],\n",
       "       [ 0.76595292],\n",
       "       [ 0.9256175 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_time_step = 16\n",
    "seasonality = 30 #14\n",
    "output_time_step = [2,3,5,7,10]\n",
    "epoch = 1000\n",
    "batch_size = 32\n",
    "validation_split=0.1\n",
    "optimizer = 'adam'\n",
    "loss = 'mean_squared_error'\n",
    "dropout = 0.1\n",
    "units = 100\n",
    "dilations=[1, 2, 4, 8]\n",
    "callback = EarlyStopping(monitor = 'val_loss',\n",
    "                         min_delta = 1e-4, \n",
    "                         patience = 100, \n",
    "                         verbose=1, \n",
    "                         mode='auto', \n",
    "                         restore_best_weights=True)\n",
    "RMSE = []\n",
    "MAE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Smoothing + Normalization\n",
    "class ES(Layer):\n",
    "\n",
    "    def __init__(self, horizon, m, batch_size, time_steps, **kwargs):\n",
    "        self.horizon = horizon\n",
    "        self.m = m\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        super(ES, self).__init__(**kwargs)\n",
    "\n",
    "    # initialization of the learned parameters of exponential smoothing\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(name='alpha', shape=(1,),\n",
    "                                     initializer='uniform', trainable=True)\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(1,),\n",
    "                                     initializer='uniform', trainable=True)\n",
    "        self.init_seasonality = self.add_weight(name='init_seasonality', shape=(self.m,),\n",
    "                                                initializer=initializers.Constant(value=0.8), trainable=True)\n",
    "        self.init_seasonality_list = [K.slice(self.init_seasonality,(i,),(1,)) for i in range(self.m)]\n",
    "        self.seasonality_queue = deque(self.init_seasonality_list, self.m)\n",
    "        self.level = self.add_weight(name='init_level', shape=(1,),\n",
    "                                     initializer=initializers.Constant(value=0.8), \n",
    "                                     trainable=True)\n",
    "        super(ES, self).build(input_shape)  \n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        # extract time-series from feature vector\n",
    "        n_examples = K.int_shape(x)[0]\n",
    "        if n_examples is None:\n",
    "            n_examples = self.batch_size\n",
    "        x1 = K.slice(x,(0,0,0),(1,self.time_steps,1))\n",
    "        x1 = K.reshape(x1,(self.time_steps,))\n",
    "        x2 = K.slice(x,(1,self.time_steps-1,0),(n_examples-1,1,1))\n",
    "        x2 = K.reshape(x2,(n_examples-1,))\n",
    "        ts = K.concatenate([x1,x2])\n",
    "        \n",
    "        x_norm = []  # normalized values of time-series\n",
    "        ls = []      # coeffients for denormalization of forecasts\n",
    "        \n",
    "        l_t_minus_1 = self.level\n",
    "        \n",
    "        for i in range(n_examples+self.time_steps-1):\n",
    "        \n",
    "            # compute l_t\n",
    "            y_t = ts[i]\n",
    "            s_t = self.seasonality_queue.popleft()\n",
    "            l_t = self.alpha * y_t / s_t + (1 - self.alpha) * l_t_minus_1\n",
    "            \n",
    "            # compute s_{t+m}\n",
    "            s_t_plus_m = self.gamma * y_t / l_t + (1 - self.gamma) * s_t\n",
    "            \n",
    "            self.seasonality_queue.append(s_t_plus_m)\n",
    "            \n",
    "            # normalize y_t\n",
    "            x_norm.append(y_t / (s_t * l_t))\n",
    "\n",
    "            l_t_minus_1 = l_t\n",
    "\n",
    "            if i >= self.time_steps-1:\n",
    "                l = [l_t]*self.horizon\n",
    "                l = K.concatenate(l)\n",
    "                s = [self.seasonality_queue[i] for i in range(self.horizon)] # we assume here that horizon < m\n",
    "                s = K.concatenate(s)\n",
    "                ls_t = K.concatenate([K.expand_dims(l), K.expand_dims(s)])\n",
    "                ls.append(K.expand_dims(ls_t,axis=0))  \n",
    "       \n",
    "        self.level = l_t\n",
    "        x_norm = K.concatenate(x_norm)\n",
    "\n",
    "        # create x_out\n",
    "        x_out = []\n",
    "        for i in range(n_examples):\n",
    "            norm_features = K.slice(x_norm,(i,),(self.time_steps,))\n",
    "            norm_features = K.expand_dims(norm_features,axis=0)\n",
    "            x_out.append(norm_features)\n",
    "\n",
    "        x_out = K.concatenate(x_out, axis=0)\n",
    "        x_out = K.expand_dims(x_out)\n",
    "\n",
    "        # create tensor of denormalization coefficients \n",
    "        denorm_coeff = K.concatenate(ls, axis=0)\n",
    "        return [x_out, denorm_coeff]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], input_shape[1], input_shape[2]), (input_shape[0], self.horizon, 2)]\n",
    "    \n",
    "class Denormalization(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Denormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Denormalization, self).build(input_shape)  \n",
    "\n",
    "    def call(self, x):\n",
    "        return x[0] * x[1][:,:,0] * x[1][:,:,1]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3507, 16, 1)\n",
      "(3507, 2)\n",
      "(423, 16, 1)\n",
      "(423, 2)\n",
      "(3488, 16, 1)\n",
      "(3488, 2)\n",
      "(416, 16, 1)\n",
      "(416, 2)\n",
      "Train on 3488 samples, validate on 416 samples\n",
      "Epoch 1/1000\n",
      "3488/3488 [==============================] - 3s 948us/step - loss: 0.1051 - val_loss: 0.1021\n",
      "Epoch 2/1000\n",
      "3488/3488 [==============================] - 1s 306us/step - loss: 0.0386 - val_loss: 0.0535\n",
      "Epoch 3/1000\n",
      "3488/3488 [==============================] - 1s 319us/step - loss: 0.0243 - val_loss: 0.0362\n",
      "Epoch 4/1000\n",
      "3488/3488 [==============================] - 1s 323us/step - loss: 0.0187 - val_loss: 0.0244\n",
      "Epoch 5/1000\n",
      "3488/3488 [==============================] - 1s 323us/step - loss: 0.0152 - val_loss: 0.0163\n",
      "Epoch 6/1000\n",
      "3488/3488 [==============================] - 1s 407us/step - loss: 0.0127 - val_loss: 0.0116\n",
      "Epoch 7/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 8/1000\n",
      "3488/3488 [==============================] - 2s 551us/step - loss: 0.0098 - val_loss: 0.0100\n",
      "Epoch 9/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0090 - val_loss: 0.0096\n",
      "Epoch 10/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.0085 - val_loss: 0.0090\n",
      "Epoch 11/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0081 - val_loss: 0.0085\n",
      "Epoch 12/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0078 - val_loss: 0.0080\n",
      "Epoch 13/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 14/1000\n",
      "3488/3488 [==============================] - 2s 608us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 15/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 16/1000\n",
      "3488/3488 [==============================] - 2s 456us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 17/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0068 - val_loss: 0.0065\n",
      "Epoch 18/1000\n",
      "3488/3488 [==============================] - 2s 604us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 19/1000\n",
      "3488/3488 [==============================] - 2s 541us/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 20/1000\n",
      "3488/3488 [==============================] - 2s 449us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 21/1000\n",
      "3488/3488 [==============================] - 2s 450us/step - loss: 0.0064 - val_loss: 0.0060\n",
      "Epoch 22/1000\n",
      "3488/3488 [==============================] - 2s 455us/step - loss: 0.0063 - val_loss: 0.0059\n",
      "Epoch 23/1000\n",
      "3488/3488 [==============================] - 2s 453us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 24/1000\n",
      "3488/3488 [==============================] - 2s 454us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 25/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.0060 - val_loss: 0.0057\n",
      "Epoch 26/1000\n",
      "3488/3488 [==============================] - 2s 460us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 27/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 28/1000\n",
      "3488/3488 [==============================] - 2s 481us/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 29/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 30/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 31/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 32/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 33/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 34/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 35/1000\n",
      "3488/3488 [==============================] - 1s 372us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 36/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 37/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 38/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 39/1000\n",
      "3488/3488 [==============================] - 2s 476us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 40/1000\n",
      "3488/3488 [==============================] - 2s 560us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 41/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 42/1000\n",
      "3488/3488 [==============================] - 2s 453us/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 43/1000\n",
      "3488/3488 [==============================] - 2s 448us/step - loss: 0.0047 - val_loss: 0.0049\n",
      "Epoch 44/1000\n",
      "3488/3488 [==============================] - 2s 443us/step - loss: 0.0047 - val_loss: 0.0050\n",
      "Epoch 45/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0047 - val_loss: 0.0051\n",
      "Epoch 46/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 47/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 48/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 49/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0046 - val_loss: 0.0050\n",
      "Epoch 50/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 51/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0045 - val_loss: 0.0049\n",
      "Epoch 52/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 53/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 54/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 55/1000\n",
      "3488/3488 [==============================] - 2s 443us/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 56/1000\n",
      "3488/3488 [==============================] - 1s 351us/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 57/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0045 - val_loss: 0.0049\n",
      "Epoch 58/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 59/1000\n",
      "3488/3488 [==============================] - 2s 437us/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 60/1000\n",
      "3488/3488 [==============================] - 2s 454us/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 61/1000\n",
      "3488/3488 [==============================] - 2s 452us/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 62/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 63/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 64/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 65/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0046 - val_loss: 0.0065\n",
      "Epoch 66/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 67/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0045 - val_loss: 0.0059\n",
      "Epoch 68/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 69/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0045 - val_loss: 0.0048\n",
      "Epoch 70/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 71/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 72/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 73/1000\n",
      "3488/3488 [==============================] - 1s 418us/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 74/1000\n",
      "3488/3488 [==============================] - 1s 412us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 75/1000\n",
      "3488/3488 [==============================] - 1s 412us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 76/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 1s 412us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 77/1000\n",
      "3488/3488 [==============================] - 1s 296us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 78/1000\n",
      "3488/3488 [==============================] - 2s 437us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 79/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 80/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 81/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 82/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 83/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 84/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 85/1000\n",
      "3488/3488 [==============================] - 2s 530us/step - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 86/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 87/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 88/1000\n",
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 89/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 90/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 91/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 92/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 93/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0043 - val_loss: 0.0048\n",
      "Epoch 94/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0043 - val_loss: 0.0048\n",
      "Epoch 95/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 96/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 97/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 98/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0049 - val_loss: 0.1189\n",
      "Epoch 99/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0248 - val_loss: 0.0050\n",
      "Epoch 100/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.2728 - val_loss: 0.0311\n",
      "Epoch 101/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0106 - val_loss: 0.0076\n",
      "Epoch 102/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0056 - val_loss: 0.0073\n",
      "Epoch 103/1000\n",
      "3488/3488 [==============================] - 1s 392us/step - loss: 0.0051 - val_loss: 0.0074\n",
      "Epoch 104/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0049 - val_loss: 0.0073\n",
      "Epoch 105/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0047 - val_loss: 0.0071\n",
      "Epoch 106/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 107/1000\n",
      "3488/3488 [==============================] - 2s 541us/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 108/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 109/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0045 - val_loss: 0.0069\n",
      "Epoch 110/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0045 - val_loss: 0.0067\n",
      "Epoch 111/1000\n",
      "3488/3488 [==============================] - 1s 355us/step - loss: 0.0045 - val_loss: 0.0066\n",
      "Epoch 112/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.0045 - val_loss: 0.0064\n",
      "Epoch 113/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0045 - val_loss: 0.0063\n",
      "Epoch 114/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0044 - val_loss: 0.0062\n",
      "Epoch 115/1000\n",
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0044 - val_loss: 0.0061\n",
      "Epoch 116/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0044 - val_loss: 0.0061\n",
      "Epoch 117/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0044 - val_loss: 0.0060\n",
      "Epoch 118/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.0044 - val_loss: 0.0060\n",
      "Epoch 119/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.0044 - val_loss: 0.0060\n",
      "Epoch 120/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 121/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 122/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 123/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 124/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 125/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0044 - val_loss: 0.0058\n",
      "Epoch 126/1000\n",
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0044 - val_loss: 0.0058\n",
      "Epoch 127/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.0044 - val_loss: 0.0057\n",
      "Epoch 128/1000\n",
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0044 - val_loss: 0.0057\n",
      "Epoch 129/1000\n",
      "3488/3488 [==============================] - 2s 476us/step - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 130/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 131/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 132/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 133/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 134/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0044 - val_loss: 0.0053\n",
      "Epoch 135/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 136/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 137/1000\n",
      "3488/3488 [==============================] - 2s 476us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 138/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 139/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 140/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 141/1000\n",
      "3488/3488 [==============================] - 2s 483us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 142/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 143/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 144/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 145/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 146/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 147/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 148/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 149/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 150/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 151/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 152/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 153/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 154/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 155/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00155: early stopping\n",
      "(422, 16, 1)\n",
      "(422, 2)\n",
      "(416, 16, 1)\n",
      "(416, 2)\n",
      "(3506, 16, 1)\n",
      "(3506, 3)\n",
      "(422, 16, 1)\n",
      "(422, 3)\n",
      "(3488, 16, 1)\n",
      "(3488, 3)\n",
      "(416, 16, 1)\n",
      "(416, 3)\n",
      "Train on 3488 samples, validate on 416 samples\n",
      "Epoch 1/1000\n",
      "3488/3488 [==============================] - 4s 1ms/step - loss: 0.2857 - val_loss: 0.0329\n",
      "Epoch 2/1000\n",
      "3488/3488 [==============================] - 2s 440us/step - loss: 0.1539 - val_loss: 0.0159\n",
      "Epoch 3/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.1287 - val_loss: 0.0457\n",
      "Epoch 4/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0862 - val_loss: 0.0175\n",
      "Epoch 5/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0699 - val_loss: 0.0344\n",
      "Epoch 6/1000\n",
      "3488/3488 [==============================] - 2s 554us/step - loss: 0.0657 - val_loss: 0.0250\n",
      "Epoch 7/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0595 - val_loss: 0.0189\n",
      "Epoch 8/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.0591 - val_loss: 0.0130\n",
      "Epoch 9/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 1.3144 - val_loss: 0.0467\n",
      "Epoch 10/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.2066 - val_loss: 0.0172\n",
      "Epoch 11/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.1056 - val_loss: 0.0201\n",
      "Epoch 12/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0878 - val_loss: 0.0214\n",
      "Epoch 13/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0788 - val_loss: 0.0222\n",
      "Epoch 14/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.0724 - val_loss: 0.0225\n",
      "Epoch 15/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0671 - val_loss: 0.0225\n",
      "Epoch 16/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0625 - val_loss: 0.0222\n",
      "Epoch 17/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0583 - val_loss: 0.0216\n",
      "Epoch 18/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0542 - val_loss: 0.0208\n",
      "Epoch 19/1000\n",
      "3488/3488 [==============================] - 1s 334us/step - loss: 0.0497 - val_loss: 0.0196\n",
      "Epoch 20/1000\n",
      "3488/3488 [==============================] - 1s 321us/step - loss: 0.0584 - val_loss: 0.0290\n",
      "Epoch 21/1000\n",
      "3488/3488 [==============================] - 1s 401us/step - loss: 0.0511 - val_loss: 0.0218\n",
      "Epoch 22/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0474 - val_loss: 0.0197\n",
      "Epoch 23/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.1783 - val_loss: 0.0170\n",
      "Epoch 24/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1173 - val_loss: 0.0176\n",
      "Epoch 25/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.1163 - val_loss: 0.0179\n",
      "Epoch 26/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.5561 - val_loss: 0.1053\n",
      "Epoch 27/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.8767 - val_loss: 0.3812\n",
      "Epoch 28/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.8729 - val_loss: 0.7773\n",
      "Epoch 29/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.4152 - val_loss: 0.1187\n",
      "Epoch 30/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 11.9618 - val_loss: 1.1389\n",
      "Epoch 31/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.8857 - val_loss: 1.0091\n",
      "Epoch 32/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.4511 - val_loss: 0.2680\n",
      "Epoch 33/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.1871 - val_loss: 0.0462\n",
      "Epoch 34/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.1868 - val_loss: 0.0451\n",
      "Epoch 35/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1718 - val_loss: 0.0457\n",
      "Epoch 36/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1602 - val_loss: 0.0460\n",
      "Epoch 37/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.1513 - val_loss: 0.0463\n",
      "Epoch 38/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.1438 - val_loss: 0.0465\n",
      "Epoch 39/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.1374 - val_loss: 0.0467\n",
      "Epoch 40/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.1316 - val_loss: 0.0469\n",
      "Epoch 41/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.1263 - val_loss: 0.0474\n",
      "Epoch 42/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.1219 - val_loss: 0.0480\n",
      "Epoch 43/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1181 - val_loss: 0.0485\n",
      "Epoch 44/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.1146 - val_loss: 0.0490\n",
      "Epoch 45/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.1114 - val_loss: 0.0495\n",
      "Epoch 46/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.1086 - val_loss: 0.0499\n",
      "Epoch 47/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.1061 - val_loss: 0.0503\n",
      "Epoch 48/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.1038 - val_loss: 0.0507\n",
      "Epoch 49/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.1017 - val_loss: 0.0511\n",
      "Epoch 50/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0997 - val_loss: 0.0514\n",
      "Epoch 51/1000\n",
      "3488/3488 [==============================] - 1s 348us/step - loss: 0.0978 - val_loss: 0.0517\n",
      "Epoch 52/1000\n",
      "3488/3488 [==============================] - 1s 344us/step - loss: 0.0960 - val_loss: 0.0520\n",
      "Epoch 53/1000\n",
      "3488/3488 [==============================] - 1s 344us/step - loss: 0.0943 - val_loss: 0.0521\n",
      "Epoch 54/1000\n",
      "3488/3488 [==============================] - 1s 346us/step - loss: 0.0927 - val_loss: 0.0522\n",
      "Epoch 55/1000\n",
      "3488/3488 [==============================] - 1s 345us/step - loss: 0.0912 - val_loss: 0.0523\n",
      "Epoch 56/1000\n",
      "3488/3488 [==============================] - 1s 377us/step - loss: 0.0897 - val_loss: 0.0522\n",
      "Epoch 57/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0883 - val_loss: 0.0520\n",
      "Epoch 58/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0870 - val_loss: 0.0518\n",
      "Epoch 59/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.0857 - val_loss: 0.0516\n",
      "Epoch 60/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0845 - val_loss: 0.0513\n",
      "Epoch 61/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0833 - val_loss: 0.0509\n",
      "Epoch 62/1000\n",
      "3488/3488 [==============================] - 2s 553us/step - loss: 0.0823 - val_loss: 0.0506\n",
      "Epoch 63/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.0811 - val_loss: 0.0502\n",
      "Epoch 64/1000\n",
      "3488/3488 [==============================] - 2s 538us/step - loss: 0.0807 - val_loss: 0.0500\n",
      "Epoch 65/1000\n",
      "3488/3488 [==============================] - 2s 554us/step - loss: 0.0798 - val_loss: 0.0493\n",
      "Epoch 66/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0788 - val_loss: 0.0497\n",
      "Epoch 67/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0770 - val_loss: 0.0489\n",
      "Epoch 68/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0820 - val_loss: 0.0503\n",
      "Epoch 69/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0848 - val_loss: 0.0517\n",
      "Epoch 70/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0780 - val_loss: 0.0531\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 1s 348us/step - loss: 0.0738 - val_loss: 0.0535\n",
      "Epoch 72/1000\n",
      "3488/3488 [==============================] - 1s 345us/step - loss: 0.0720 - val_loss: 0.0538\n",
      "Epoch 73/1000\n",
      "3488/3488 [==============================] - 1s 345us/step - loss: 0.0704 - val_loss: 0.0539\n",
      "Epoch 74/1000\n",
      "3488/3488 [==============================] - 1s 375us/step - loss: 0.0789 - val_loss: 0.0546\n",
      "Epoch 75/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0746 - val_loss: 0.0542\n",
      "Epoch 76/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0685 - val_loss: 0.0535\n",
      "Epoch 77/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0716 - val_loss: 0.0541\n",
      "Epoch 78/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0708 - val_loss: 0.0532\n",
      "Epoch 79/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0654 - val_loss: 0.0527\n",
      "Epoch 80/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0666 - val_loss: 0.0524\n",
      "Epoch 81/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0647 - val_loss: 0.0510\n",
      "Epoch 82/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0660 - val_loss: 0.0522\n",
      "Epoch 83/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0642 - val_loss: 0.0488\n",
      "Epoch 84/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0658 - val_loss: 0.0516\n",
      "Epoch 85/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0636 - val_loss: 0.0476\n",
      "Epoch 86/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0652 - val_loss: 0.0509\n",
      "Epoch 87/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0625 - val_loss: 0.0465\n",
      "Epoch 88/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0602 - val_loss: 0.0488\n",
      "Epoch 89/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0646 - val_loss: 0.0453\n",
      "Epoch 90/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 10.4719 - val_loss: 0.6497\n",
      "Epoch 91/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 13.4589 - val_loss: 0.7151\n",
      "Epoch 92/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.2757 - val_loss: 0.5646\n",
      "Epoch 93/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.2229 - val_loss: 0.4283\n",
      "Epoch 94/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.1928 - val_loss: 0.3175\n",
      "Epoch 95/1000\n",
      "3488/3488 [==============================] - 1s 357us/step - loss: 0.1572 - val_loss: 0.2455\n",
      "Epoch 96/1000\n",
      "3488/3488 [==============================] - 1s 355us/step - loss: 0.1305 - val_loss: 0.1926\n",
      "Epoch 97/1000\n",
      "3488/3488 [==============================] - 1s 369us/step - loss: 0.0970 - val_loss: 0.1570\n",
      "Epoch 98/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.0914 - val_loss: 0.1090\n",
      "Epoch 99/1000\n",
      "3488/3488 [==============================] - 2s 543us/step - loss: 0.0614 - val_loss: 0.0858\n",
      "Epoch 100/1000\n",
      "3488/3488 [==============================] - 2s 551us/step - loss: 0.0636 - val_loss: 0.0664\n",
      "Epoch 101/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0551 - val_loss: 0.0568\n",
      "Epoch 102/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.0441 - val_loss: 0.0541\n",
      "Epoch 103/1000\n",
      "3488/3488 [==============================] - 1s 400us/step - loss: 0.0428 - val_loss: 0.0516\n",
      "Epoch 104/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0388 - val_loss: 0.0518\n",
      "Epoch 105/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0391 - val_loss: 0.0513\n",
      "Epoch 106/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0366 - val_loss: 0.0517\n",
      "Epoch 107/1000\n",
      "3488/3488 [==============================] - 2s 528us/step - loss: 0.0357 - val_loss: 0.0518\n",
      "Epoch 108/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0343 - val_loss: 0.0520\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00108: early stopping\n",
      "(421, 16, 1)\n",
      "(421, 3)\n",
      "(416, 16, 1)\n",
      "(416, 3)\n",
      "(3504, 16, 1)\n",
      "(3504, 5)\n",
      "(420, 16, 1)\n",
      "(420, 5)\n",
      "(3488, 16, 1)\n",
      "(3488, 5)\n",
      "(416, 16, 1)\n",
      "(416, 5)\n",
      "Train on 3488 samples, validate on 416 samples\n",
      "Epoch 1/1000\n",
      "3488/3488 [==============================] - 5s 1ms/step - loss: 51.2296 - val_loss: 1.1873\n",
      "Epoch 2/1000\n",
      "3488/3488 [==============================] - 2s 436us/step - loss: 0.6898 - val_loss: 1.1677\n",
      "Epoch 3/1000\n",
      "3488/3488 [==============================] - 2s 453us/step - loss: 0.6798 - val_loss: 1.1439\n",
      "Epoch 4/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.6679 - val_loss: 1.1166\n",
      "Epoch 5/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.6544 - val_loss: 1.0848\n",
      "Epoch 6/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.6386 - val_loss: 1.0462\n",
      "Epoch 7/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.6193 - val_loss: 0.9955\n",
      "Epoch 8/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.5937 - val_loss: 0.9215\n",
      "Epoch 9/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.5559 - val_loss: 0.7983\n",
      "Epoch 10/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.4918 - val_loss: 0.5262\n",
      "Epoch 11/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.3951 - val_loss: 0.1297\n",
      "Epoch 12/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.3821 - val_loss: 0.0893\n",
      "Epoch 13/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.3388 - val_loss: 0.0720\n",
      "Epoch 14/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.3100 - val_loss: 0.0610\n",
      "Epoch 15/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.2881 - val_loss: 0.0531\n",
      "Epoch 16/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.2702 - val_loss: 0.0471\n",
      "Epoch 17/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.2549 - val_loss: 0.0425\n",
      "Epoch 18/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.2424 - val_loss: 0.0390\n",
      "Epoch 19/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.2347 - val_loss: 0.0368\n",
      "Epoch 20/1000\n",
      "3488/3488 [==============================] - 2s 481us/step - loss: 0.2266 - val_loss: 0.0350\n",
      "Epoch 21/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.2200 - val_loss: 0.0335\n",
      "Epoch 22/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.2127 - val_loss: 0.0322\n",
      "Epoch 23/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.2072 - val_loss: 0.0311\n",
      "Epoch 24/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.2011 - val_loss: 0.0301\n",
      "Epoch 25/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.1942 - val_loss: 0.0291\n",
      "Epoch 26/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.1882 - val_loss: 0.0284\n",
      "Epoch 27/1000\n",
      "3488/3488 [==============================] - 2s 481us/step - loss: 0.1861 - val_loss: 0.0280\n",
      "Epoch 28/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.1835 - val_loss: 0.0275\n",
      "Epoch 29/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.1739 - val_loss: 0.0269\n",
      "Epoch 30/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.1747 - val_loss: 0.0268\n",
      "Epoch 31/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.1764 - val_loss: 0.0266\n",
      "Epoch 32/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.1662 - val_loss: 0.0262\n",
      "Epoch 33/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.1586 - val_loss: 0.0259\n",
      "Epoch 34/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.1623 - val_loss: 0.0260\n",
      "Epoch 35/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.1659 - val_loss: 0.0259\n",
      "Epoch 36/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.1556 - val_loss: 0.0257\n",
      "Epoch 37/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.1484 - val_loss: 0.0257\n",
      "Epoch 38/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.1601 - val_loss: 0.0262\n",
      "Epoch 39/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.1583 - val_loss: 0.0260\n",
      "Epoch 40/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.1483 - val_loss: 0.0261\n",
      "Epoch 41/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.1402 - val_loss: 0.0263\n",
      "Epoch 42/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 17.8556 - val_loss: 0.6392\n",
      "Epoch 43/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.5910 - val_loss: 0.4843\n",
      "Epoch 44/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.5763 - val_loss: 0.4250\n",
      "Epoch 45/1000\n",
      "3488/3488 [==============================] - 2s 558us/step - loss: 0.5658 - val_loss: 0.3951\n",
      "Epoch 46/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.5622 - val_loss: 0.3814\n",
      "Epoch 47/1000\n",
      "3488/3488 [==============================] - 2s 483us/step - loss: 0.5599 - val_loss: 0.3748\n",
      "Epoch 48/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.5580 - val_loss: 0.3711\n",
      "Epoch 49/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.5562 - val_loss: 0.3687\n",
      "Epoch 50/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.5548 - val_loss: 0.3668\n",
      "Epoch 51/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.5534 - val_loss: 0.3652\n",
      "Epoch 52/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.5522 - val_loss: 0.3639\n",
      "Epoch 53/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.5511 - val_loss: 0.3627\n",
      "Epoch 54/1000\n",
      "3488/3488 [==============================] - 2s 483us/step - loss: 0.5502 - val_loss: 0.3617\n",
      "Epoch 55/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.5493 - val_loss: 0.3609\n",
      "Epoch 56/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.5485 - val_loss: 0.3602\n",
      "Epoch 57/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.5478 - val_loss: 0.3597\n",
      "Epoch 58/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.5470 - val_loss: 0.3592\n",
      "Epoch 59/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.5464 - val_loss: 0.3590\n",
      "Epoch 60/1000\n",
      "3488/3488 [==============================] - 1s 295us/step - loss: 0.5459 - val_loss: 0.3590\n",
      "Epoch 61/1000\n",
      "3488/3488 [==============================] - 1s 381us/step - loss: 0.5456 - val_loss: 0.3593\n",
      "Epoch 62/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.5454 - val_loss: 0.3599\n",
      "Epoch 63/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.5449 - val_loss: 0.3605\n",
      "Epoch 64/1000\n",
      "3488/3488 [==============================] - 1s 414us/step - loss: 0.5449 - val_loss: 0.3614\n",
      "Epoch 65/1000\n",
      "3488/3488 [==============================] - 2s 456us/step - loss: 0.5449 - val_loss: 0.3626\n",
      "Epoch 66/1000\n",
      "3488/3488 [==============================] - 2s 453us/step - loss: 0.5451 - val_loss: 0.3642\n",
      "Epoch 67/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.5450 - val_loss: 0.3658\n",
      "Epoch 68/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.5452 - val_loss: 0.3677\n",
      "Epoch 69/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.5456 - val_loss: 0.3700\n",
      "Epoch 70/1000\n",
      "3488/3488 [==============================] - 2s 444us/step - loss: 0.5459 - val_loss: 0.3726\n",
      "Epoch 71/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.5461 - val_loss: 0.3754\n",
      "Epoch 72/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.5459 - val_loss: 0.3782\n",
      "Epoch 73/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.5458 - val_loss: 0.3815\n",
      "Epoch 74/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.5445 - val_loss: 0.3857\n",
      "Epoch 75/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.5409 - val_loss: 0.3961\n",
      "Epoch 76/1000\n",
      "3488/3488 [==============================] - 2s 467us/step - loss: 0.5329 - val_loss: 0.4323\n",
      "Epoch 77/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.5204 - val_loss: 0.4751\n",
      "Epoch 78/1000\n",
      "3488/3488 [==============================] - 2s 451us/step - loss: 0.5033 - val_loss: 0.5001\n",
      "Epoch 79/1000\n",
      "3488/3488 [==============================] - 2s 446us/step - loss: 0.4809 - val_loss: 0.4539\n",
      "Epoch 80/1000\n",
      "3488/3488 [==============================] - 2s 471us/step - loss: 0.4679 - val_loss: 0.4349\n",
      "Epoch 81/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.4578 - val_loss: 0.3883\n",
      "Epoch 82/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.4577 - val_loss: 0.3801\n",
      "Epoch 83/1000\n",
      "3488/3488 [==============================] - 2s 445us/step - loss: 0.4342 - val_loss: 0.3305\n",
      "Epoch 84/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.4082 - val_loss: 0.3198\n",
      "Epoch 85/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.3672 - val_loss: 0.2865\n",
      "Epoch 86/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.3585 - val_loss: 0.2890\n",
      "Epoch 87/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.3274 - val_loss: 0.2735\n",
      "Epoch 88/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.3160 - val_loss: 0.2713\n",
      "Epoch 89/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.2967 - val_loss: 0.2657\n",
      "Epoch 90/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.2873 - val_loss: 0.2611\n",
      "Epoch 91/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.2841 - val_loss: 0.2551\n",
      "Epoch 92/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.2968 - val_loss: 0.2480\n",
      "Epoch 93/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.3256 - val_loss: 0.2630\n",
      "Epoch 94/1000\n",
      "3488/3488 [==============================] - 1s 342us/step - loss: 0.2811 - val_loss: 0.2702\n",
      "Epoch 95/1000\n",
      "3488/3488 [==============================] - 1s 350us/step - loss: 0.2430 - val_loss: 0.2495\n",
      "Epoch 96/1000\n",
      "3488/3488 [==============================] - 2s 452us/step - loss: 0.2465 - val_loss: 0.2341\n",
      "Epoch 97/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.2685 - val_loss: 0.2248\n",
      "Epoch 98/1000\n",
      "3488/3488 [==============================] - 2s 454us/step - loss: 0.3153 - val_loss: 0.2742\n",
      "Epoch 99/1000\n",
      "3488/3488 [==============================] - 2s 459us/step - loss: 0.3807 - val_loss: 0.3167\n",
      "Epoch 100/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.2406 - val_loss: 0.2256\n",
      "Epoch 101/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.2355 - val_loss: 0.2097\n",
      "Epoch 102/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.2210 - val_loss: 0.2026\n",
      "Epoch 103/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.2135 - val_loss: 0.1961\n",
      "Epoch 104/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.2066 - val_loss: 0.1901\n",
      "Epoch 105/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.2006 - val_loss: 0.1849\n",
      "Epoch 106/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1952 - val_loss: 0.1805\n",
      "Epoch 107/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.1902 - val_loss: 0.1764\n",
      "Epoch 108/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.1851 - val_loss: 0.1718\n",
      "Epoch 109/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.1807 - val_loss: 0.1677\n",
      "Epoch 110/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.1762 - val_loss: 0.1627\n",
      "Epoch 111/1000\n",
      "3488/3488 [==============================] - 1s 405us/step - loss: 0.1722 - val_loss: 0.1578\n",
      "Epoch 112/1000\n",
      "3488/3488 [==============================] - 1s 374us/step - loss: 0.1688 - val_loss: 0.1536\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.1648 - val_loss: 0.1486\n",
      "Epoch 114/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.1624 - val_loss: 0.1451\n",
      "Epoch 115/1000\n",
      "3488/3488 [==============================] - 2s 542us/step - loss: 0.1570 - val_loss: 0.1389\n",
      "Epoch 116/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.1592 - val_loss: 0.1395\n",
      "Epoch 117/1000\n",
      "3488/3488 [==============================] - 2s 433us/step - loss: 0.1478 - val_loss: 0.1247\n",
      "Epoch 118/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.1592 - val_loss: 0.1349\n",
      "Epoch 119/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.1575 - val_loss: 0.1189\n",
      "Epoch 120/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.1434 - val_loss: 0.1160\n",
      "Epoch 121/1000\n",
      "3488/3488 [==============================] - 2s 458us/step - loss: 0.1379 - val_loss: 0.1118\n",
      "Epoch 122/1000\n",
      "3488/3488 [==============================] - 2s 457us/step - loss: 0.1374 - val_loss: 0.1108\n",
      "Epoch 123/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.1317 - val_loss: 0.1058\n",
      "Epoch 124/1000\n",
      "3488/3488 [==============================] - 2s 460us/step - loss: 0.1336 - val_loss: 0.1063\n",
      "Epoch 125/1000\n",
      "3488/3488 [==============================] - 1s 419us/step - loss: 0.1248 - val_loss: 0.0990\n",
      "Epoch 126/1000\n",
      "3488/3488 [==============================] - 2s 476us/step - loss: 0.1299 - val_loss: 0.1017\n",
      "Epoch 127/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.1195 - val_loss: 0.0941\n",
      "Epoch 128/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.1229 - val_loss: 0.0939\n",
      "Epoch 129/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.1149 - val_loss: 0.0892\n",
      "Epoch 130/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1160 - val_loss: 0.0881\n",
      "Epoch 131/1000\n",
      "3488/3488 [==============================] - 2s 467us/step - loss: 0.1099 - val_loss: 0.0843\n",
      "Epoch 132/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.1078 - val_loss: 0.0797\n",
      "Epoch 133/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.1032 - val_loss: 0.0779\n",
      "Epoch 134/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0955 - val_loss: 0.0718\n",
      "Epoch 135/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.0957 - val_loss: 0.0768\n",
      "Epoch 136/1000\n",
      "3488/3488 [==============================] - 1s 396us/step - loss: 0.0963 - val_loss: 0.0699\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00136: early stopping\n",
      "(419, 16, 1)\n",
      "(419, 5)\n",
      "(416, 16, 1)\n",
      "(416, 5)\n",
      "(3502, 16, 1)\n",
      "(3502, 7)\n",
      "(418, 16, 1)\n",
      "(418, 7)\n",
      "(3488, 16, 1)\n",
      "(3488, 7)\n",
      "(416, 16, 1)\n",
      "(416, 7)\n",
      "Train on 3488 samples, validate on 416 samples\n",
      "Epoch 1/1000\n",
      "3488/3488 [==============================] - 4s 1ms/step - loss: 25.9396 - val_loss: 1.4440\n",
      "Epoch 2/1000\n",
      "3488/3488 [==============================] - 2s 447us/step - loss: 0.8354 - val_loss: 1.7735\n",
      "Epoch 3/1000\n",
      "3488/3488 [==============================] - 2s 551us/step - loss: 0.8126 - val_loss: 1.6350\n",
      "Epoch 4/1000\n",
      "3488/3488 [==============================] - 2s 561us/step - loss: 0.7925 - val_loss: 2.0248\n",
      "Epoch 5/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.7704 - val_loss: 3.4862\n",
      "Epoch 6/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.7449 - val_loss: 190.7440\n",
      "Epoch 7/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.7121 - val_loss: 2.2366\n",
      "Epoch 8/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.6534 - val_loss: 1.2986\n",
      "Epoch 9/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.5000 - val_loss: 1.2272\n",
      "Epoch 10/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.3818 - val_loss: 1.3109\n",
      "Epoch 11/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.3714 - val_loss: 1.2666\n",
      "Epoch 12/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.3635 - val_loss: 1.2426\n",
      "Epoch 13/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.3563 - val_loss: 1.2159\n",
      "Epoch 14/1000\n",
      "3488/3488 [==============================] - 2s 440us/step - loss: 0.3492 - val_loss: 1.1881\n",
      "Epoch 15/1000\n",
      "3488/3488 [==============================] - 1s 386us/step - loss: 0.3419 - val_loss: 1.1593\n",
      "Epoch 16/1000\n",
      "3488/3488 [==============================] - 2s 444us/step - loss: 0.3340 - val_loss: 1.1274\n",
      "Epoch 17/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.3248 - val_loss: 1.0912\n",
      "Epoch 18/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.3138 - val_loss: 1.0479\n",
      "Epoch 19/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.2994 - val_loss: 0.9925\n",
      "Epoch 20/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.2761 - val_loss: 0.9157\n",
      "Epoch 21/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.2417 - val_loss: 0.8424\n",
      "Epoch 22/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.2194 - val_loss: 0.7904\n",
      "Epoch 23/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.2081 - val_loss: 0.7548\n",
      "Epoch 24/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.2000 - val_loss: 0.7314\n",
      "Epoch 25/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.1933 - val_loss: 0.7119\n",
      "Epoch 26/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.1875 - val_loss: 0.6989\n",
      "Epoch 27/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.1821 - val_loss: 0.6838\n",
      "Epoch 28/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.1770 - val_loss: 0.6711\n",
      "Epoch 29/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.1720 - val_loss: 0.6561\n",
      "Epoch 30/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.1672 - val_loss: 0.6424\n",
      "Epoch 31/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.1626 - val_loss: 0.6274\n",
      "Epoch 32/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.1583 - val_loss: 0.6334\n",
      "Epoch 33/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.1542 - val_loss: 0.6327\n",
      "Epoch 34/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.1502 - val_loss: 0.6435\n",
      "Epoch 35/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.1466 - val_loss: 0.7111\n",
      "Epoch 36/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.1432 - val_loss: 0.7713\n",
      "Epoch 37/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.1398 - val_loss: 0.8486\n",
      "Epoch 38/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.1366 - val_loss: 0.9697\n",
      "Epoch 39/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.1336 - val_loss: 1.0946\n",
      "Epoch 40/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.1307 - val_loss: 1.2180\n",
      "Epoch 41/1000\n",
      "3488/3488 [==============================] - 2s 575us/step - loss: 0.1280 - val_loss: 1.3232\n",
      "Epoch 42/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.1254 - val_loss: 1.4863\n",
      "Epoch 43/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.1228 - val_loss: 1.6212\n",
      "Epoch 44/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.1205 - val_loss: 2.0077\n",
      "Epoch 45/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.1182 - val_loss: 1.9857\n",
      "Epoch 46/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.1161 - val_loss: 3.2107\n",
      "Epoch 47/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.1140 - val_loss: 2.4831\n",
      "Epoch 48/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.1121 - val_loss: 3.4040\n",
      "Epoch 49/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.1102 - val_loss: 3.2548\n",
      "Epoch 50/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.1082 - val_loss: 4.1696\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.1063 - val_loss: 4.9872\n",
      "Epoch 52/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.1042 - val_loss: 7.2691\n",
      "Epoch 53/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.1022 - val_loss: 8.3326\n",
      "Epoch 54/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.1004 - val_loss: 12.6213\n",
      "Epoch 55/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0987 - val_loss: 70.0993\n",
      "Epoch 56/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0971 - val_loss: 76.1900\n",
      "Epoch 57/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0957 - val_loss: 4.1867\n",
      "Epoch 58/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0945 - val_loss: 6.2981\n",
      "Epoch 59/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0935 - val_loss: 0.3270\n",
      "Epoch 60/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0942 - val_loss: 0.3602\n",
      "Epoch 61/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.1031 - val_loss: 1.1791\n",
      "Epoch 62/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.1483 - val_loss: 0.4109\n",
      "Epoch 63/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.1338 - val_loss: 0.3044\n",
      "Epoch 64/1000\n",
      "3488/3488 [==============================] - 2s 483us/step - loss: 0.0935 - val_loss: 0.2634\n",
      "Epoch 65/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0745 - val_loss: 0.2528\n",
      "Epoch 66/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0720 - val_loss: 0.2468\n",
      "Epoch 67/1000\n",
      "3488/3488 [==============================] - 1s 342us/step - loss: 0.0702 - val_loss: 0.2381\n",
      "Epoch 68/1000\n",
      "3488/3488 [==============================] - 1s 341us/step - loss: 0.0686 - val_loss: 0.2324\n",
      "Epoch 69/1000\n",
      "3488/3488 [==============================] - 1s 354us/step - loss: 0.0671 - val_loss: 0.2273\n",
      "Epoch 70/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0658 - val_loss: 0.2227\n",
      "Epoch 71/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0646 - val_loss: 0.2182\n",
      "Epoch 72/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0634 - val_loss: 0.2137\n",
      "Epoch 73/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0622 - val_loss: 0.2094\n",
      "Epoch 74/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0611 - val_loss: 0.2052\n",
      "Epoch 75/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.0600 - val_loss: 0.2013\n",
      "Epoch 76/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0589 - val_loss: 0.1975\n",
      "Epoch 77/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0579 - val_loss: 0.1938\n",
      "Epoch 78/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.0570 - val_loss: 0.1904\n",
      "Epoch 79/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0561 - val_loss: 0.1869\n",
      "Epoch 80/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0552 - val_loss: 0.1837\n",
      "Epoch 81/1000\n",
      "3488/3488 [==============================] - 1s 387us/step - loss: 0.0543 - val_loss: 0.1805\n",
      "Epoch 82/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0535 - val_loss: 0.1774\n",
      "Epoch 83/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0527 - val_loss: 0.1744\n",
      "Epoch 84/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0519 - val_loss: 0.1714\n",
      "Epoch 85/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0512 - val_loss: 0.1683\n",
      "Epoch 86/1000\n",
      "3488/3488 [==============================] - 2s 528us/step - loss: 0.0505 - val_loss: 0.1663\n",
      "Epoch 87/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0498 - val_loss: 0.1628\n",
      "Epoch 88/1000\n",
      "3488/3488 [==============================] - 2s 545us/step - loss: 0.0491 - val_loss: 0.1590\n",
      "Epoch 89/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0483 - val_loss: 0.1558\n",
      "Epoch 90/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0476 - val_loss: 0.1533\n",
      "Epoch 91/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0469 - val_loss: 0.1510\n",
      "Epoch 92/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0463 - val_loss: 0.1487\n",
      "Epoch 93/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0457 - val_loss: 0.1464\n",
      "Epoch 94/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0451 - val_loss: 0.1442\n",
      "Epoch 95/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0446 - val_loss: 0.1419\n",
      "Epoch 96/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0440 - val_loss: 0.1397\n",
      "Epoch 97/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0434 - val_loss: 0.1376\n",
      "Epoch 98/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0429 - val_loss: 0.1355\n",
      "Epoch 99/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0423 - val_loss: 0.1335\n",
      "Epoch 100/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.0418 - val_loss: 0.1315\n",
      "Epoch 101/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0413 - val_loss: 0.1295\n",
      "Epoch 102/1000\n",
      "3488/3488 [==============================] - 1s 377us/step - loss: 0.0408 - val_loss: 0.1276\n",
      "Epoch 103/1000\n",
      "3488/3488 [==============================] - 2s 461us/step - loss: 0.0402 - val_loss: 0.1259\n",
      "Epoch 104/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0397 - val_loss: 0.1241\n",
      "Epoch 105/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.0393 - val_loss: 0.1226\n",
      "Epoch 106/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0388 - val_loss: 0.1210\n",
      "Epoch 107/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0383 - val_loss: 0.1197\n",
      "Epoch 108/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0379 - val_loss: 0.1181\n",
      "Epoch 109/1000\n",
      "3488/3488 [==============================] - 2s 504us/step - loss: 0.0374 - val_loss: 0.1167\n",
      "Epoch 110/1000\n",
      "3488/3488 [==============================] - 2s 540us/step - loss: 0.0369 - val_loss: 0.1153\n",
      "Epoch 111/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0365 - val_loss: 0.1141\n",
      "Epoch 112/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0360 - val_loss: 0.1129\n",
      "Epoch 113/1000\n",
      "3488/3488 [==============================] - 2s 528us/step - loss: 0.0356 - val_loss: 0.1117\n",
      "Epoch 114/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.0352 - val_loss: 0.1108\n",
      "Epoch 115/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0349 - val_loss: 0.1094\n",
      "Epoch 116/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0345 - val_loss: 0.1076\n",
      "Epoch 117/1000\n",
      "3488/3488 [==============================] - 2s 559us/step - loss: 0.0340 - val_loss: 0.1068\n",
      "Epoch 118/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.0337 - val_loss: 0.1062\n",
      "Epoch 119/1000\n",
      "3488/3488 [==============================] - 1s 404us/step - loss: 0.0335 - val_loss: 0.1065\n",
      "Epoch 120/1000\n",
      "3488/3488 [==============================] - 1s 366us/step - loss: 0.0334 - val_loss: 0.1055\n",
      "Epoch 121/1000\n",
      "3488/3488 [==============================] - 1s 417us/step - loss: 0.0332 - val_loss: 0.1030\n",
      "Epoch 122/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.0331 - val_loss: 0.1068\n",
      "Epoch 123/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0327 - val_loss: 0.0975\n",
      "Epoch 124/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0317 - val_loss: 0.0958\n",
      "Epoch 125/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0314 - val_loss: 0.0953\n",
      "Epoch 126/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0310 - val_loss: 0.0934\n",
      "Epoch 127/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0309 - val_loss: 0.0970\n",
      "Epoch 128/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0309 - val_loss: 0.0959\n",
      "Epoch 129/1000\n",
      "3488/3488 [==============================] - 2s 479us/step - loss: 0.0306 - val_loss: 0.1210\n",
      "Epoch 130/1000\n",
      "3488/3488 [==============================] - 1s 423us/step - loss: 0.0302 - val_loss: 0.1015\n",
      "Epoch 131/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0300 - val_loss: 0.1192\n",
      "Epoch 132/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0299 - val_loss: 0.1530\n",
      "Epoch 133/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0313 - val_loss: 0.2455\n",
      "Epoch 134/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0348 - val_loss: 1.7952\n",
      "Epoch 135/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 5841.5401 - val_loss: 1.2403\n",
      "Epoch 136/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.3155 - val_loss: 0.4458\n",
      "Epoch 137/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.2120 - val_loss: 0.4484\n",
      "Epoch 138/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.2060 - val_loss: 0.4463\n",
      "Epoch 139/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.2032 - val_loss: 0.4439\n",
      "Epoch 140/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.2006 - val_loss: 0.4412\n",
      "Epoch 141/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.1981 - val_loss: 0.4384\n",
      "Epoch 142/1000\n",
      "3488/3488 [==============================] - 2s 434us/step - loss: 0.1957 - val_loss: 0.4355\n",
      "Epoch 143/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.1934 - val_loss: 0.4324\n",
      "Epoch 144/1000\n",
      "3488/3488 [==============================] - 2s 538us/step - loss: 0.1912 - val_loss: 0.4293\n",
      "Epoch 145/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.1891 - val_loss: 0.4261\n",
      "Epoch 146/1000\n",
      "3488/3488 [==============================] - 2s 557us/step - loss: 0.1870 - val_loss: 0.4227\n",
      "Epoch 147/1000\n",
      "3488/3488 [==============================] - 2s 547us/step - loss: 0.1850 - val_loss: 0.4192\n",
      "Epoch 148/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.1830 - val_loss: 0.4156\n",
      "Epoch 149/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.1811 - val_loss: 0.4119\n",
      "Epoch 150/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.1792 - val_loss: 0.4081\n",
      "Epoch 151/1000\n",
      "3488/3488 [==============================] - 2s 551us/step - loss: 0.1773 - val_loss: 0.4042\n",
      "Epoch 152/1000\n",
      "3488/3488 [==============================] - 2s 587us/step - loss: 0.1755 - val_loss: 0.4002\n",
      "Epoch 153/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.1736 - val_loss: 0.3961\n",
      "Epoch 154/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.1718 - val_loss: 0.3919\n",
      "Epoch 155/1000\n",
      "3488/3488 [==============================] - 2s 581us/step - loss: 0.1700 - val_loss: 0.3877\n",
      "Epoch 156/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.1682 - val_loss: 0.3834\n",
      "Epoch 157/1000\n",
      "3488/3488 [==============================] - 1s 276us/step - loss: 0.1664 - val_loss: 0.3790\n",
      "Epoch 158/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.1646 - val_loss: 0.3746\n",
      "Epoch 159/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.1629 - val_loss: 0.3702\n",
      "Epoch 160/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.1612 - val_loss: 0.3658\n",
      "Epoch 161/1000\n",
      "3488/3488 [==============================] - 2s 530us/step - loss: 0.1595 - val_loss: 0.3614\n",
      "Epoch 162/1000\n",
      "3488/3488 [==============================] - 2s 533us/step - loss: 0.1578 - val_loss: 0.3571\n",
      "Epoch 163/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.1562 - val_loss: 0.3530\n",
      "Epoch 164/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.1546 - val_loss: 0.3490\n",
      "Epoch 165/1000\n",
      "3488/3488 [==============================] - 2s 528us/step - loss: 0.1530 - val_loss: 0.3451\n",
      "Epoch 166/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.1516 - val_loss: 0.3415\n",
      "Epoch 167/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.1502 - val_loss: 0.3382\n",
      "Epoch 168/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.1488 - val_loss: 0.3351\n",
      "Epoch 169/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.1475 - val_loss: 0.3322\n",
      "Epoch 170/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.1463 - val_loss: 0.3296\n",
      "Epoch 171/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.1452 - val_loss: 0.3273\n",
      "Epoch 172/1000\n",
      "3488/3488 [==============================] - 2s 528us/step - loss: 0.1441 - val_loss: 0.3251\n",
      "Epoch 173/1000\n",
      "3488/3488 [==============================] - 2s 541us/step - loss: 0.1431 - val_loss: 0.3232\n",
      "Epoch 174/1000\n",
      "3488/3488 [==============================] - 2s 539us/step - loss: 0.1421 - val_loss: 0.3214\n",
      "Epoch 175/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.1412 - val_loss: 0.3198\n",
      "Epoch 176/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.1404 - val_loss: 0.3183\n",
      "Epoch 177/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.1396 - val_loss: 0.3170\n",
      "Epoch 178/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.1389 - val_loss: 0.3157\n",
      "Epoch 179/1000\n",
      "3488/3488 [==============================] - 1s 408us/step - loss: 0.1382 - val_loss: 0.3145\n",
      "Epoch 180/1000\n",
      "3488/3488 [==============================] - 1s 362us/step - loss: 0.1376 - val_loss: 0.3133\n",
      "Epoch 181/1000\n",
      "3488/3488 [==============================] - 1s 414us/step - loss: 0.1369 - val_loss: 0.3122\n",
      "Epoch 182/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.1364 - val_loss: 0.3112\n",
      "Epoch 183/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.1358 - val_loss: 0.3101\n",
      "Epoch 184/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.1353 - val_loss: 0.3091\n",
      "Epoch 185/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.1348 - val_loss: 0.3081\n",
      "Epoch 186/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.1343 - val_loss: 0.3071\n",
      "Epoch 187/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.1338 - val_loss: 0.3061\n",
      "Epoch 188/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.1333 - val_loss: 0.3051\n",
      "Epoch 189/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.1329 - val_loss: 0.3040\n",
      "Epoch 190/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.1324 - val_loss: 0.3030\n",
      "Epoch 191/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.1320 - val_loss: 0.3019\n",
      "Epoch 192/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.1316 - val_loss: 0.3008\n",
      "Epoch 193/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.1312 - val_loss: 0.2997\n",
      "Epoch 194/1000\n",
      "3488/3488 [==============================] - 2s 544us/step - loss: 0.1307 - val_loss: 0.2985\n",
      "Epoch 195/1000\n",
      "3488/3488 [==============================] - 2s 553us/step - loss: 0.1303 - val_loss: 0.2973\n",
      "Epoch 196/1000\n",
      "3488/3488 [==============================] - 2s 565us/step - loss: 0.1299 - val_loss: 0.2961\n",
      "Epoch 197/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.1295 - val_loss: 0.2949\n",
      "Epoch 198/1000\n",
      "3488/3488 [==============================] - 2s 528us/step - loss: 0.1291 - val_loss: 0.2936\n",
      "Epoch 199/1000\n",
      "3488/3488 [==============================] - 2s 535us/step - loss: 0.1286 - val_loss: 0.2924\n",
      "Epoch 200/1000\n",
      "3488/3488 [==============================] - 1s 405us/step - loss: 0.1282 - val_loss: 0.2911\n",
      "Epoch 201/1000\n",
      "3488/3488 [==============================] - 2s 435us/step - loss: 0.1278 - val_loss: 0.2898\n",
      "Epoch 202/1000\n",
      "3488/3488 [==============================] - 1s 409us/step - loss: 0.1273 - val_loss: 0.2885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.1268 - val_loss: 0.2872\n",
      "Epoch 204/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.1264 - val_loss: 0.2859\n",
      "Epoch 205/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.1259 - val_loss: 0.2846\n",
      "Epoch 206/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.1254 - val_loss: 0.2834\n",
      "Epoch 207/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.1249 - val_loss: 0.2822\n",
      "Epoch 208/1000\n",
      "3488/3488 [==============================] - 2s 537us/step - loss: 0.1244 - val_loss: 0.2810\n",
      "Epoch 209/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.1238 - val_loss: 0.2799\n",
      "Epoch 210/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1233 - val_loss: 0.2788\n",
      "Epoch 211/1000\n",
      "3488/3488 [==============================] - 2s 500us/step - loss: 0.1227 - val_loss: 0.2778\n",
      "Epoch 212/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.1222 - val_loss: 0.2768\n",
      "Epoch 213/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.1216 - val_loss: 0.2759\n",
      "Epoch 214/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.1210 - val_loss: 0.2750\n",
      "Epoch 215/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.1204 - val_loss: 0.2741\n",
      "Epoch 216/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.1198 - val_loss: 0.2732\n",
      "Epoch 217/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.1192 - val_loss: 0.2724\n",
      "Epoch 218/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.1186 - val_loss: 0.2716\n",
      "Epoch 219/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.1179 - val_loss: 0.2707\n",
      "Epoch 220/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.1172 - val_loss: 0.2699\n",
      "Epoch 221/1000\n",
      "3488/3488 [==============================] - 2s 457us/step - loss: 0.1165 - val_loss: 0.2691\n",
      "Epoch 222/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.1157 - val_loss: 0.2683\n",
      "Epoch 223/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.1150 - val_loss: 0.2674\n",
      "Epoch 224/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.1142 - val_loss: 0.2666\n",
      "Epoch 225/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.1133 - val_loss: 0.2656\n",
      "Epoch 226/1000\n",
      "3488/3488 [==============================] - 2s 433us/step - loss: 0.1125 - val_loss: 0.2647\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00226: early stopping\n",
      "(417, 16, 1)\n",
      "(417, 7)\n",
      "(416, 16, 1)\n",
      "(416, 7)\n",
      "(3499, 16, 1)\n",
      "(3499, 10)\n",
      "(415, 16, 1)\n",
      "(415, 10)\n",
      "(3488, 16, 1)\n",
      "(3488, 10)\n",
      "(384, 16, 1)\n",
      "(384, 10)\n",
      "Train on 3488 samples, validate on 384 samples\n",
      "Epoch 1/1000\n",
      "3488/3488 [==============================] - 5s 1ms/step - loss: 0.5926 - val_loss: 0.1218\n",
      "Epoch 2/1000\n",
      "3488/3488 [==============================] - 2s 454us/step - loss: 0.3773 - val_loss: 0.0697\n",
      "Epoch 3/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.1869 - val_loss: 0.0381\n",
      "Epoch 4/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.6014 - val_loss: 0.2678\n",
      "Epoch 5/1000\n",
      "3488/3488 [==============================] - 2s 547us/step - loss: 0.2559 - val_loss: 0.1569\n",
      "Epoch 6/1000\n",
      "3488/3488 [==============================] - 2s 535us/step - loss: 0.1819 - val_loss: 0.1048\n",
      "Epoch 7/1000\n",
      "3488/3488 [==============================] - 2s 557us/step - loss: 0.1486 - val_loss: 0.0602\n",
      "Epoch 8/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.1256 - val_loss: 0.0379\n",
      "Epoch 9/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.1129 - val_loss: 0.0312\n",
      "Epoch 10/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.1051 - val_loss: 0.0313\n",
      "Epoch 11/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.0996 - val_loss: 0.0312\n",
      "Epoch 12/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.0950 - val_loss: 0.0309\n",
      "Epoch 13/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0909 - val_loss: 0.0308\n",
      "Epoch 14/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0873 - val_loss: 0.0308\n",
      "Epoch 15/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0841 - val_loss: 0.0310\n",
      "Epoch 16/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0812 - val_loss: 0.0312\n",
      "Epoch 17/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0785 - val_loss: 0.0313\n",
      "Epoch 18/1000\n",
      "3488/3488 [==============================] - 2s 607us/step - loss: 0.0762 - val_loss: 0.0309\n",
      "Epoch 19/1000\n",
      "3488/3488 [==============================] - 2s 542us/step - loss: 0.0739 - val_loss: 0.0299\n",
      "Epoch 20/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.0717 - val_loss: 0.0287\n",
      "Epoch 21/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0697 - val_loss: 0.0276\n",
      "Epoch 22/1000\n",
      "3488/3488 [==============================] - 2s 608us/step - loss: 0.0679 - val_loss: 0.0267\n",
      "Epoch 23/1000\n",
      "3488/3488 [==============================] - 2s 544us/step - loss: 0.0661 - val_loss: 0.0261\n",
      "Epoch 24/1000\n",
      "3488/3488 [==============================] - 2s 580us/step - loss: 0.0645 - val_loss: 0.0257\n",
      "Epoch 25/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0630 - val_loss: 0.0254\n",
      "Epoch 26/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0615 - val_loss: 0.0252\n",
      "Epoch 27/1000\n",
      "3488/3488 [==============================] - 2s 533us/step - loss: 0.0602 - val_loss: 0.0250\n",
      "Epoch 28/1000\n",
      "3488/3488 [==============================] - 2s 570us/step - loss: 0.0589 - val_loss: 0.0249\n",
      "Epoch 29/1000\n",
      "3488/3488 [==============================] - 2s 561us/step - loss: 0.0577 - val_loss: 0.0248\n",
      "Epoch 30/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.0566 - val_loss: 0.0247\n",
      "Epoch 31/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0554 - val_loss: 0.0247\n",
      "Epoch 32/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0542 - val_loss: 0.0247\n",
      "Epoch 33/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0531 - val_loss: 0.0247\n",
      "Epoch 34/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0520 - val_loss: 0.0247\n",
      "Epoch 35/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.0510 - val_loss: 0.0248\n",
      "Epoch 36/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0502 - val_loss: 0.0248\n",
      "Epoch 37/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0493 - val_loss: 0.0249\n",
      "Epoch 38/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.0486 - val_loss: 0.0250\n",
      "Epoch 39/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.0477 - val_loss: 0.0250\n",
      "Epoch 40/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.0470 - val_loss: 0.0251\n",
      "Epoch 41/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.0461 - val_loss: 0.0249\n",
      "Epoch 42/1000\n",
      "3488/3488 [==============================] - 2s 545us/step - loss: 0.0453 - val_loss: 0.0248\n",
      "Epoch 43/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0446 - val_loss: 0.0252\n",
      "Epoch 44/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0440 - val_loss: 0.0255\n",
      "Epoch 45/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.0434 - val_loss: 0.0256\n",
      "Epoch 46/1000\n",
      "3488/3488 [==============================] - 2s 538us/step - loss: 0.0428 - val_loss: 0.0256\n",
      "Epoch 47/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.0421 - val_loss: 0.0239\n",
      "Epoch 48/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0415 - val_loss: 0.0223\n",
      "Epoch 49/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0407 - val_loss: 0.0238\n",
      "Epoch 50/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0399 - val_loss: 0.0250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0391 - val_loss: 0.0231\n",
      "Epoch 52/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0382 - val_loss: 0.0229\n",
      "Epoch 53/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0376 - val_loss: 0.0232\n",
      "Epoch 54/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0367 - val_loss: 0.0230\n",
      "Epoch 55/1000\n",
      "3488/3488 [==============================] - 2s 569us/step - loss: 0.0361 - val_loss: 0.0228\n",
      "Epoch 56/1000\n",
      "3488/3488 [==============================] - 2s 540us/step - loss: 0.0351 - val_loss: 0.0231\n",
      "Epoch 57/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0346 - val_loss: 0.0233\n",
      "Epoch 58/1000\n",
      "3488/3488 [==============================] - 2s 559us/step - loss: 0.0338 - val_loss: 0.0229\n",
      "Epoch 59/1000\n",
      "3488/3488 [==============================] - 2s 586us/step - loss: 0.0330 - val_loss: 0.0244\n",
      "Epoch 60/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0326 - val_loss: 0.0280\n",
      "Epoch 61/1000\n",
      "3488/3488 [==============================] - 1s 417us/step - loss: 0.0330 - val_loss: 0.0244\n",
      "Epoch 62/1000\n",
      "3488/3488 [==============================] - 1s 390us/step - loss: 0.0315 - val_loss: 0.0277\n",
      "Epoch 63/1000\n",
      "3488/3488 [==============================] - 1s 392us/step - loss: 0.0304 - val_loss: 0.0288\n",
      "Epoch 64/1000\n",
      "3488/3488 [==============================] - 1s 389us/step - loss: 0.0295 - val_loss: 0.0275\n",
      "Epoch 65/1000\n",
      "3488/3488 [==============================] - 1s 392us/step - loss: 0.0287 - val_loss: 0.0274\n",
      "Epoch 66/1000\n",
      "3488/3488 [==============================] - 1s 393us/step - loss: 0.0280 - val_loss: 0.0266\n",
      "Epoch 67/1000\n",
      "3488/3488 [==============================] - 2s 450us/step - loss: 0.0273 - val_loss: 0.0264\n",
      "Epoch 68/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0266 - val_loss: 0.0269\n",
      "Epoch 69/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0262 - val_loss: 0.0265\n",
      "Epoch 70/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 71/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0253 - val_loss: 0.0282\n",
      "Epoch 72/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0248 - val_loss: 0.0273\n",
      "Epoch 73/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0241 - val_loss: 0.0268\n",
      "Epoch 74/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0239 - val_loss: 0.0271\n",
      "Epoch 75/1000\n",
      "3488/3488 [==============================] - 2s 456us/step - loss: 0.0228 - val_loss: 0.0278\n",
      "Epoch 76/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0220 - val_loss: 0.0281\n",
      "Epoch 77/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0216 - val_loss: 0.0269\n",
      "Epoch 78/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0210 - val_loss: 0.0261\n",
      "Epoch 79/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0202 - val_loss: 0.0292\n",
      "Epoch 80/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0192 - val_loss: 0.0280\n",
      "Epoch 81/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.0174 - val_loss: 0.0273\n",
      "Epoch 82/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0162 - val_loss: 0.0277\n",
      "Epoch 83/1000\n",
      "3488/3488 [==============================] - 2s 547us/step - loss: 0.0160 - val_loss: 0.0279\n",
      "Epoch 84/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0157 - val_loss: 0.0277\n",
      "Epoch 85/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0157 - val_loss: 0.0272\n",
      "Epoch 86/1000\n",
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0155 - val_loss: 0.0269\n",
      "Epoch 87/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.0155 - val_loss: 0.0267\n",
      "Epoch 88/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0153 - val_loss: 0.0266\n",
      "Epoch 89/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0153 - val_loss: 0.0259\n",
      "Epoch 90/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0152 - val_loss: 0.0261\n",
      "Epoch 91/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0152 - val_loss: 0.0253\n",
      "Epoch 92/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0151 - val_loss: 0.0260\n",
      "Epoch 93/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0151 - val_loss: 0.0246\n",
      "Epoch 94/1000\n",
      "3488/3488 [==============================] - 1s 423us/step - loss: 0.0150 - val_loss: 0.0254\n",
      "Epoch 95/1000\n",
      "3488/3488 [==============================] - 2s 440us/step - loss: 0.0150 - val_loss: 0.0243\n",
      "Epoch 96/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0149 - val_loss: 0.0254\n",
      "Epoch 97/1000\n",
      "3488/3488 [==============================] - 2s 545us/step - loss: 0.0149 - val_loss: 0.0242\n",
      "Epoch 98/1000\n",
      "3488/3488 [==============================] - 2s 541us/step - loss: 0.0149 - val_loss: 0.0250\n",
      "Epoch 99/1000\n",
      "3488/3488 [==============================] - 2s 567us/step - loss: 0.0148 - val_loss: 0.0240\n",
      "Epoch 100/1000\n",
      "3488/3488 [==============================] - 2s 550us/step - loss: 0.0148 - val_loss: 0.0248\n",
      "Epoch 101/1000\n",
      "3488/3488 [==============================] - 2s 534us/step - loss: 0.0147 - val_loss: 0.0239\n",
      "Epoch 102/1000\n",
      "3488/3488 [==============================] - 2s 445us/step - loss: 0.0147 - val_loss: 0.0247\n",
      "Epoch 103/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0147 - val_loss: 0.0238\n",
      "Epoch 104/1000\n",
      "3488/3488 [==============================] - 2s 547us/step - loss: 0.0147 - val_loss: 0.0246\n",
      "Epoch 105/1000\n",
      "3488/3488 [==============================] - 2s 557us/step - loss: 0.0146 - val_loss: 0.0238\n",
      "Epoch 106/1000\n",
      "3488/3488 [==============================] - 2s 540us/step - loss: 0.0146 - val_loss: 0.0246\n",
      "Epoch 107/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0145 - val_loss: 0.0239\n",
      "Epoch 108/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0146 - val_loss: 0.0248\n",
      "Epoch 109/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0144 - val_loss: 0.0242\n",
      "Epoch 110/1000\n",
      "3488/3488 [==============================] - 2s 556us/step - loss: 0.0145 - val_loss: 0.0249\n",
      "Epoch 111/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.0144 - val_loss: 0.0242\n",
      "Epoch 112/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.0144 - val_loss: 0.0247\n",
      "Epoch 113/1000\n",
      "3488/3488 [==============================] - 2s 542us/step - loss: 0.0144 - val_loss: 0.0244\n",
      "Epoch 114/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.0143 - val_loss: 0.0242\n",
      "Epoch 115/1000\n",
      "3488/3488 [==============================] - 2s 547us/step - loss: 0.0143 - val_loss: 0.0240\n",
      "Epoch 116/1000\n",
      "3488/3488 [==============================] - 2s 540us/step - loss: 0.0142 - val_loss: 0.0242\n",
      "Epoch 117/1000\n",
      "3488/3488 [==============================] - 2s 559us/step - loss: 0.0142 - val_loss: 0.0237\n",
      "Epoch 118/1000\n",
      "3488/3488 [==============================] - 2s 547us/step - loss: 0.0142 - val_loss: 0.0242\n",
      "Epoch 119/1000\n",
      "3488/3488 [==============================] - 2s 445us/step - loss: 0.0142 - val_loss: 0.0238\n",
      "Epoch 120/1000\n",
      "3488/3488 [==============================] - 1s 358us/step - loss: 0.0141 - val_loss: 0.0242\n",
      "Epoch 121/1000\n",
      "3488/3488 [==============================] - 1s 411us/step - loss: 0.0141 - val_loss: 0.0237\n",
      "Epoch 122/1000\n",
      "3488/3488 [==============================] - 2s 565us/step - loss: 0.0141 - val_loss: 0.0241\n",
      "Epoch 123/1000\n",
      "3488/3488 [==============================] - 2s 549us/step - loss: 0.0141 - val_loss: 0.0233\n",
      "Epoch 124/1000\n",
      "3488/3488 [==============================] - 2s 548us/step - loss: 0.0140 - val_loss: 0.0237\n",
      "Epoch 125/1000\n",
      "3488/3488 [==============================] - 2s 580us/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 126/1000\n",
      "3488/3488 [==============================] - 2s 434us/step - loss: 0.0140 - val_loss: 0.0236\n",
      "Epoch 127/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.0139 - val_loss: 0.0231\n",
      "Epoch 128/1000\n",
      "3488/3488 [==============================] - 2s 537us/step - loss: 0.0139 - val_loss: 0.0233\n",
      "Epoch 129/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.0139 - val_loss: 0.0230\n",
      "Epoch 130/1000\n",
      "3488/3488 [==============================] - 2s 533us/step - loss: 0.0139 - val_loss: 0.0231\n",
      "Epoch 131/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0139 - val_loss: 0.0229\n",
      "Epoch 132/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0138 - val_loss: 0.0231\n",
      "Epoch 133/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0138 - val_loss: 0.0228\n",
      "Epoch 134/1000\n",
      "3488/3488 [==============================] - 1s 303us/step - loss: 0.0138 - val_loss: 0.0233\n",
      "Epoch 135/1000\n",
      "3488/3488 [==============================] - 2s 476us/step - loss: 0.0138 - val_loss: 0.0227\n",
      "Epoch 136/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0138 - val_loss: 0.0230\n",
      "Epoch 137/1000\n",
      "3488/3488 [==============================] - 2s 602us/step - loss: 0.0138 - val_loss: 0.0225\n",
      "Epoch 138/1000\n",
      "3488/3488 [==============================] - 2s 619us/step - loss: 0.0137 - val_loss: 0.0228\n",
      "Epoch 139/1000\n",
      "3488/3488 [==============================] - 2s 574us/step - loss: 0.0137 - val_loss: 0.0225\n",
      "Epoch 140/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0137 - val_loss: 0.0225\n",
      "Epoch 141/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.0137 - val_loss: 0.0223\n",
      "Epoch 142/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0137 - val_loss: 0.0226\n",
      "Epoch 143/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0137 - val_loss: 0.0224\n",
      "Epoch 144/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0137 - val_loss: 0.0223\n",
      "Epoch 145/1000\n",
      "3488/3488 [==============================] - 2s 455us/step - loss: 0.0137 - val_loss: 0.0224\n",
      "Epoch 146/1000\n",
      "3488/3488 [==============================] - 2s 461us/step - loss: 0.0137 - val_loss: 0.0222\n",
      "Epoch 147/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0137 - val_loss: 0.0222\n",
      "Epoch 148/1000\n",
      "3488/3488 [==============================] - 2s 454us/step - loss: 0.0136 - val_loss: 0.0218\n",
      "Epoch 149/1000\n",
      "3488/3488 [==============================] - 2s 459us/step - loss: 0.0136 - val_loss: 0.0221\n",
      "Epoch 150/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0136 - val_loss: 0.0215\n",
      "Epoch 151/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0136 - val_loss: 0.0224\n",
      "Epoch 152/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0136 - val_loss: 0.0216\n",
      "Epoch 153/1000\n",
      "3488/3488 [==============================] - 2s 467us/step - loss: 0.0136 - val_loss: 0.0225\n",
      "Epoch 154/1000\n",
      "3488/3488 [==============================] - 2s 496us/step - loss: 0.0136 - val_loss: 0.0216\n",
      "Epoch 155/1000\n",
      "3488/3488 [==============================] - 2s 460us/step - loss: 0.0136 - val_loss: 0.0223\n",
      "Epoch 156/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0136 - val_loss: 0.0214\n",
      "Epoch 157/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.0136 - val_loss: 0.0222\n",
      "Epoch 158/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0136 - val_loss: 0.0211\n",
      "Epoch 159/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0136 - val_loss: 0.0227\n",
      "Epoch 160/1000\n",
      "3488/3488 [==============================] - 2s 556us/step - loss: 0.0136 - val_loss: 0.0213\n",
      "Epoch 161/1000\n",
      "3488/3488 [==============================] - 2s 551us/step - loss: 0.0136 - val_loss: 0.0225\n",
      "Epoch 162/1000\n",
      "3488/3488 [==============================] - 2s 436us/step - loss: 0.0135 - val_loss: 0.0213\n",
      "Epoch 163/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0136 - val_loss: 0.0222\n",
      "Epoch 164/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.0135 - val_loss: 0.0211\n",
      "Epoch 165/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0136 - val_loss: 0.0225\n",
      "Epoch 166/1000\n",
      "3488/3488 [==============================] - 2s 467us/step - loss: 0.0136 - val_loss: 0.0208\n",
      "Epoch 167/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0136 - val_loss: 0.0232\n",
      "Epoch 168/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0135 - val_loss: 0.0212\n",
      "Epoch 169/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.0135 - val_loss: 0.0224\n",
      "Epoch 170/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0135 - val_loss: 0.0211\n",
      "Epoch 171/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0135 - val_loss: 0.0217\n",
      "Epoch 172/1000\n",
      "3488/3488 [==============================] - 2s 467us/step - loss: 0.0135 - val_loss: 0.0206\n",
      "Epoch 173/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0135 - val_loss: 0.0224\n",
      "Epoch 174/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0136 - val_loss: 0.0209\n",
      "Epoch 175/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.0136 - val_loss: 0.0230\n",
      "Epoch 176/1000\n",
      "3488/3488 [==============================] - 1s 422us/step - loss: 0.0135 - val_loss: 0.0210\n",
      "Epoch 177/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0136 - val_loss: 0.0226\n",
      "Epoch 178/1000\n",
      "3488/3488 [==============================] - 2s 558us/step - loss: 0.0134 - val_loss: 0.0210\n",
      "Epoch 179/1000\n",
      "3488/3488 [==============================] - 2s 541us/step - loss: 0.0135 - val_loss: 0.0216\n",
      "Epoch 180/1000\n",
      "3488/3488 [==============================] - 2s 559us/step - loss: 0.0135 - val_loss: 0.0207\n",
      "Epoch 181/1000\n",
      "3488/3488 [==============================] - 2s 557us/step - loss: 0.0136 - val_loss: 0.0226\n",
      "Epoch 182/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0136 - val_loss: 0.0213\n",
      "Epoch 183/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.0135 - val_loss: 0.0223\n",
      "Epoch 184/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0134 - val_loss: 0.0212\n",
      "Epoch 185/1000\n",
      "3488/3488 [==============================] - 1s 342us/step - loss: 0.0134 - val_loss: 0.0212\n",
      "Epoch 186/1000\n",
      "3488/3488 [==============================] - 1s 416us/step - loss: 0.0134 - val_loss: 0.0205\n",
      "Epoch 187/1000\n",
      "3488/3488 [==============================] - 2s 478us/step - loss: 0.0134 - val_loss: 0.0219\n",
      "Epoch 188/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0134 - val_loss: 0.0210\n",
      "Epoch 189/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0136 - val_loss: 0.0222\n",
      "Epoch 190/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0135 - val_loss: 0.0209\n",
      "Epoch 191/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 192/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0134 - val_loss: 0.0211\n",
      "Epoch 193/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.0134 - val_loss: 0.0214\n",
      "Epoch 194/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0134 - val_loss: 0.0205\n",
      "Epoch 195/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0135 - val_loss: 0.0221\n",
      "Epoch 196/1000\n",
      "3488/3488 [==============================] - 1s 307us/step - loss: 0.0135 - val_loss: 0.0209\n",
      "Epoch 197/1000\n",
      "3488/3488 [==============================] - 2s 451us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 198/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0134 - val_loss: 0.0210\n",
      "Epoch 199/1000\n",
      "3488/3488 [==============================] - 2s 459us/step - loss: 0.0134 - val_loss: 0.0215\n",
      "Epoch 200/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0134 - val_loss: 0.0205\n",
      "Epoch 201/1000\n",
      "3488/3488 [==============================] - 2s 522us/step - loss: 0.0136 - val_loss: 0.0220\n",
      "Epoch 202/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0135 - val_loss: 0.0212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0134 - val_loss: 0.0219\n",
      "Epoch 204/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0133 - val_loss: 0.0213\n",
      "Epoch 205/1000\n",
      "3488/3488 [==============================] - 1s 421us/step - loss: 0.0134 - val_loss: 0.0212\n",
      "Epoch 206/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0134 - val_loss: 0.0209\n",
      "Epoch 207/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0135 - val_loss: 0.0216\n",
      "Epoch 208/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0135 - val_loss: 0.0208\n",
      "Epoch 209/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.0134 - val_loss: 0.0220\n",
      "Epoch 210/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.0133 - val_loss: 0.0210\n",
      "Epoch 211/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.0134 - val_loss: 0.0219\n",
      "Epoch 212/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0134 - val_loss: 0.0204\n",
      "Epoch 213/1000\n",
      "3488/3488 [==============================] - 1s 362us/step - loss: 0.0136 - val_loss: 0.0224\n",
      "Epoch 214/1000\n",
      "3488/3488 [==============================] - 2s 452us/step - loss: 0.0135 - val_loss: 0.0207\n",
      "Epoch 215/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0134 - val_loss: 0.0221\n",
      "Epoch 216/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0133 - val_loss: 0.0206\n",
      "Epoch 217/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 218/1000\n",
      "3488/3488 [==============================] - 2s 556us/step - loss: 0.0133 - val_loss: 0.0206\n",
      "Epoch 219/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0134 - val_loss: 0.0223\n",
      "Epoch 220/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0133 - val_loss: 0.0207\n",
      "Epoch 221/1000\n",
      "3488/3488 [==============================] - 2s 563us/step - loss: 0.0133 - val_loss: 0.0220\n",
      "Epoch 222/1000\n",
      "3488/3488 [==============================] - 2s 537us/step - loss: 0.0133 - val_loss: 0.0206\n",
      "Epoch 223/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.0134 - val_loss: 0.0217\n",
      "Epoch 224/1000\n",
      "3488/3488 [==============================] - 2s 550us/step - loss: 0.0134 - val_loss: 0.0217\n",
      "Epoch 225/1000\n",
      "3488/3488 [==============================] - 2s 537us/step - loss: 0.0134 - val_loss: 0.0214\n",
      "Epoch 226/1000\n",
      "3488/3488 [==============================] - 2s 530us/step - loss: 0.0135 - val_loss: 0.0216\n",
      "Epoch 227/1000\n",
      "3488/3488 [==============================] - 2s 577us/step - loss: 0.0134 - val_loss: 0.0217\n",
      "Epoch 228/1000\n",
      "3488/3488 [==============================] - 2s 536us/step - loss: 0.0133 - val_loss: 0.0208\n",
      "Epoch 229/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0133 - val_loss: 0.0220\n",
      "Epoch 230/1000\n",
      "3488/3488 [==============================] - 1s 282us/step - loss: 0.0132 - val_loss: 0.0206\n",
      "Epoch 231/1000\n",
      "3488/3488 [==============================] - 2s 437us/step - loss: 0.0134 - val_loss: 0.0225\n",
      "Epoch 232/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.0132 - val_loss: 0.0210\n",
      "Epoch 233/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0133 - val_loss: 0.0220\n",
      "Epoch 234/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0132 - val_loss: 0.0210\n",
      "Epoch 235/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.0133 - val_loss: 0.0224\n",
      "Epoch 236/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.0132 - val_loss: 0.0204\n",
      "Epoch 237/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0209 - val_loss: 0.0204\n",
      "Epoch 238/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0143 - val_loss: 0.0227\n",
      "Epoch 239/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0141 - val_loss: 0.0233\n",
      "Epoch 240/1000\n",
      "3488/3488 [==============================] - 2s 542us/step - loss: 0.0140 - val_loss: 0.0226\n",
      "Epoch 241/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0138 - val_loss: 0.0216\n",
      "Epoch 242/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0139 - val_loss: 0.0202\n",
      "Epoch 243/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0137 - val_loss: 0.0221\n",
      "Epoch 244/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0136 - val_loss: 0.0213\n",
      "Epoch 245/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0135 - val_loss: 0.0219\n",
      "Epoch 246/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0135 - val_loss: 0.0213\n",
      "Epoch 247/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0134 - val_loss: 0.0215\n",
      "Epoch 248/1000\n",
      "3488/3488 [==============================] - 1s 302us/step - loss: 0.0134 - val_loss: 0.0212\n",
      "Epoch 249/1000\n",
      "3488/3488 [==============================] - 2s 447us/step - loss: 0.0134 - val_loss: 0.0215\n",
      "Epoch 250/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0134 - val_loss: 0.0210\n",
      "Epoch 251/1000\n",
      "3488/3488 [==============================] - 2s 548us/step - loss: 0.0133 - val_loss: 0.0217\n",
      "Epoch 252/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0134 - val_loss: 0.0205\n",
      "Epoch 253/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.0133 - val_loss: 0.0222\n",
      "Epoch 254/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0134 - val_loss: 0.0208\n",
      "Epoch 255/1000\n",
      "3488/3488 [==============================] - 2s 557us/step - loss: 0.0134 - val_loss: 0.0221\n",
      "Epoch 256/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0135 - val_loss: 0.0208\n",
      "Epoch 257/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.0133 - val_loss: 0.0223\n",
      "Epoch 258/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0135 - val_loss: 0.0205\n",
      "Epoch 259/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0134 - val_loss: 0.0219\n",
      "Epoch 260/1000\n",
      "3488/3488 [==============================] - 2s 486us/step - loss: 0.0132 - val_loss: 0.0218\n",
      "Epoch 261/1000\n",
      "3488/3488 [==============================] - 1s 385us/step - loss: 0.0132 - val_loss: 0.0218\n",
      "Epoch 262/1000\n",
      "3488/3488 [==============================] - 2s 458us/step - loss: 0.0134 - val_loss: 0.0212\n",
      "Epoch 263/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0133 - val_loss: 0.0218\n",
      "Epoch 264/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0135 - val_loss: 0.0208\n",
      "Epoch 265/1000\n",
      "3488/3488 [==============================] - 2s 475us/step - loss: 0.0136 - val_loss: 0.0226\n",
      "Epoch 266/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0136 - val_loss: 0.0213\n",
      "Epoch 267/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0641 - val_loss: 0.0227\n",
      "Epoch 268/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0184 - val_loss: 0.0243\n",
      "Epoch 269/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.0170 - val_loss: 0.0254\n",
      "Epoch 270/1000\n",
      "3488/3488 [==============================] - 2s 534us/step - loss: 0.0168 - val_loss: 0.0216\n",
      "Epoch 271/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0158 - val_loss: 0.0219\n",
      "Epoch 272/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0154 - val_loss: 0.0217\n",
      "Epoch 273/1000\n",
      "3488/3488 [==============================] - 1s 415us/step - loss: 0.0152 - val_loss: 0.0215\n",
      "Epoch 274/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0150 - val_loss: 0.0214\n",
      "Epoch 275/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0148 - val_loss: 0.0214\n",
      "Epoch 276/1000\n",
      "3488/3488 [==============================] - 2s 438us/step - loss: 0.0148 - val_loss: 0.0214\n",
      "Epoch 277/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0147 - val_loss: 0.0211\n",
      "Epoch 278/1000\n",
      "3488/3488 [==============================] - 2s 449us/step - loss: 0.0146 - val_loss: 0.0211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0145 - val_loss: 0.0212\n",
      "Epoch 280/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0145 - val_loss: 0.0211\n",
      "Epoch 281/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0144 - val_loss: 0.0211\n",
      "Epoch 282/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0144 - val_loss: 0.0212\n",
      "Epoch 283/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0143 - val_loss: 0.0213\n",
      "Epoch 284/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0143 - val_loss: 0.0212\n",
      "Epoch 285/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0142 - val_loss: 0.0213\n",
      "Epoch 286/1000\n",
      "3488/3488 [==============================] - 1s 407us/step - loss: 0.0142 - val_loss: 0.0215\n",
      "Epoch 287/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0143 - val_loss: 0.0211\n",
      "Epoch 288/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0143 - val_loss: 0.0206\n",
      "Epoch 289/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0141 - val_loss: 0.0214\n",
      "Epoch 290/1000\n",
      "3488/3488 [==============================] - 2s 438us/step - loss: 0.0141 - val_loss: 0.0215\n",
      "Epoch 291/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0140 - val_loss: 0.0214\n",
      "Epoch 292/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0140 - val_loss: 0.0214\n",
      "Epoch 293/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0140 - val_loss: 0.0216\n",
      "Epoch 294/1000\n",
      "3488/3488 [==============================] - 2s 493us/step - loss: 0.0140 - val_loss: 0.0212\n",
      "Epoch 295/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0140 - val_loss: 0.0213\n",
      "Epoch 296/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0139 - val_loss: 0.0213\n",
      "Epoch 297/1000\n",
      "3488/3488 [==============================] - 2s 593us/step - loss: 0.0139 - val_loss: 0.0216\n",
      "Epoch 298/1000\n",
      "3488/3488 [==============================] - 2s 588us/step - loss: 0.0138 - val_loss: 0.0211\n",
      "Epoch 299/1000\n",
      "3488/3488 [==============================] - 2s 621us/step - loss: 0.0138 - val_loss: 0.0214\n",
      "Epoch 300/1000\n",
      "3488/3488 [==============================] - 2s 561us/step - loss: 0.0137 - val_loss: 0.0207\n",
      "Epoch 301/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.0139 - val_loss: 0.0221\n",
      "Epoch 302/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0139 - val_loss: 0.0208\n",
      "Epoch 303/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0139 - val_loss: 0.0215\n",
      "Epoch 304/1000\n",
      "3488/3488 [==============================] - 2s 567us/step - loss: 0.0139 - val_loss: 0.0219\n",
      "Epoch 305/1000\n",
      "3488/3488 [==============================] - 2s 587us/step - loss: 0.0138 - val_loss: 0.0209\n",
      "Epoch 306/1000\n",
      "3488/3488 [==============================] - 2s 555us/step - loss: 0.0139 - val_loss: 0.0205\n",
      "Epoch 307/1000\n",
      "3488/3488 [==============================] - 2s 557us/step - loss: 0.0136 - val_loss: 0.0209\n",
      "Epoch 308/1000\n",
      "3488/3488 [==============================] - 2s 558us/step - loss: 0.0135 - val_loss: 0.0212\n",
      "Epoch 309/1000\n",
      "3488/3488 [==============================] - 2s 558us/step - loss: 0.0134 - val_loss: 0.0208\n",
      "Epoch 310/1000\n",
      "3488/3488 [==============================] - 2s 556us/step - loss: 0.0134 - val_loss: 0.0207\n",
      "Epoch 311/1000\n",
      "3488/3488 [==============================] - 2s 554us/step - loss: 0.0137 - val_loss: 0.0203\n",
      "Epoch 312/1000\n",
      "3488/3488 [==============================] - 2s 554us/step - loss: 0.0132 - val_loss: 0.0204\n",
      "Epoch 313/1000\n",
      "3488/3488 [==============================] - 2s 563us/step - loss: 0.0132 - val_loss: 0.0204\n",
      "Epoch 314/1000\n",
      "3488/3488 [==============================] - 2s 552us/step - loss: 0.0131 - val_loss: 0.0206\n",
      "Epoch 315/1000\n",
      "3488/3488 [==============================] - 2s 570us/step - loss: 0.0130 - val_loss: 0.0210\n",
      "Epoch 316/1000\n",
      "3488/3488 [==============================] - 2s 560us/step - loss: 0.0130 - val_loss: 0.0206\n",
      "Epoch 317/1000\n",
      "3488/3488 [==============================] - 2s 561us/step - loss: 0.0131 - val_loss: 0.0207\n",
      "Epoch 318/1000\n",
      "3488/3488 [==============================] - 1s 419us/step - loss: 0.0138 - val_loss: 0.0192\n",
      "Epoch 319/1000\n",
      "3488/3488 [==============================] - 1s 392us/step - loss: 0.0134 - val_loss: 0.0202\n",
      "Epoch 320/1000\n",
      "3488/3488 [==============================] - 2s 446us/step - loss: 0.0131 - val_loss: 0.0200\n",
      "Epoch 321/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0131 - val_loss: 0.0208\n",
      "Epoch 322/1000\n",
      "3488/3488 [==============================] - 2s 569us/step - loss: 0.0133 - val_loss: 0.0199\n",
      "Epoch 323/1000\n",
      "3488/3488 [==============================] - 2s 562us/step - loss: 0.0130 - val_loss: 0.0204\n",
      "Epoch 324/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0129 - val_loss: 0.0207\n",
      "Epoch 325/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0129 - val_loss: 0.0216\n",
      "Epoch 326/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0132 - val_loss: 0.0211\n",
      "Epoch 327/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0129 - val_loss: 0.0215\n",
      "Epoch 328/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0131 - val_loss: 0.0215\n",
      "Epoch 329/1000\n",
      "3488/3488 [==============================] - 2s 545us/step - loss: 0.0131 - val_loss: 0.0207\n",
      "Epoch 330/1000\n",
      "3488/3488 [==============================] - 2s 554us/step - loss: 0.0130 - val_loss: 0.0219\n",
      "Epoch 331/1000\n",
      "3488/3488 [==============================] - 2s 533us/step - loss: 0.0132 - val_loss: 0.0204\n",
      "Epoch 332/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.0134 - val_loss: 0.0214\n",
      "Epoch 333/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0129 - val_loss: 0.0213\n",
      "Epoch 334/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0130 - val_loss: 0.0240\n",
      "Epoch 335/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0130 - val_loss: 0.0208\n",
      "Epoch 336/1000\n",
      "3488/3488 [==============================] - 2s 518us/step - loss: 0.0131 - val_loss: 0.0224\n",
      "Epoch 337/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0129 - val_loss: 0.0210\n",
      "Epoch 338/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0127 - val_loss: 0.0230\n",
      "Epoch 339/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0129 - val_loss: 0.0201\n",
      "Epoch 340/1000\n",
      "3488/3488 [==============================] - 2s 568us/step - loss: 0.0129 - val_loss: 0.0240\n",
      "Epoch 341/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0135 - val_loss: 0.0201\n",
      "Epoch 342/1000\n",
      "3488/3488 [==============================] - 2s 474us/step - loss: 0.0134 - val_loss: 0.0206\n",
      "Epoch 343/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0131 - val_loss: 0.0211\n",
      "Epoch 344/1000\n",
      "3488/3488 [==============================] - 2s 549us/step - loss: 0.0128 - val_loss: 0.0214\n",
      "Epoch 345/1000\n",
      "3488/3488 [==============================] - 2s 574us/step - loss: 0.0126 - val_loss: 0.0213\n",
      "Epoch 346/1000\n",
      "3488/3488 [==============================] - 2s 568us/step - loss: 0.0125 - val_loss: 0.0214\n",
      "Epoch 347/1000\n",
      "3488/3488 [==============================] - 2s 558us/step - loss: 0.0124 - val_loss: 0.0207\n",
      "Epoch 348/1000\n",
      "3488/3488 [==============================] - 2s 556us/step - loss: 0.0124 - val_loss: 0.0219\n",
      "Epoch 349/1000\n",
      "3488/3488 [==============================] - 2s 558us/step - loss: 0.0126 - val_loss: 0.0196\n",
      "Epoch 350/1000\n",
      "3488/3488 [==============================] - 2s 550us/step - loss: 0.0128 - val_loss: 0.0205\n",
      "Epoch 351/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0125 - val_loss: 0.0203\n",
      "Epoch 352/1000\n",
      "3488/3488 [==============================] - 1s 404us/step - loss: 0.0123 - val_loss: 0.0214\n",
      "Epoch 353/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0123 - val_loss: 0.0205\n",
      "Epoch 354/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0125 - val_loss: 0.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0130 - val_loss: 0.0202\n",
      "Epoch 356/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0130 - val_loss: 0.0195\n",
      "Epoch 357/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0128 - val_loss: 0.0206\n",
      "Epoch 358/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0130 - val_loss: 0.0206\n",
      "Epoch 359/1000\n",
      "3488/3488 [==============================] - 2s 543us/step - loss: 0.0124 - val_loss: 0.0197\n",
      "Epoch 360/1000\n",
      "3488/3488 [==============================] - 2s 516us/step - loss: 0.0124 - val_loss: 0.0222\n",
      "Epoch 361/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0126 - val_loss: 0.0199\n",
      "Epoch 362/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0129 - val_loss: 0.0209\n",
      "Epoch 363/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0131 - val_loss: 0.0211\n",
      "Epoch 364/1000\n",
      "3488/3488 [==============================] - 2s 529us/step - loss: 0.0126 - val_loss: 0.0196\n",
      "Epoch 365/1000\n",
      "3488/3488 [==============================] - 2s 501us/step - loss: 0.0124 - val_loss: 0.0219\n",
      "Epoch 366/1000\n",
      "3488/3488 [==============================] - 2s 441us/step - loss: 0.0124 - val_loss: 0.0203\n",
      "Epoch 367/1000\n",
      "3488/3488 [==============================] - 1s 404us/step - loss: 0.0130 - val_loss: 0.0223\n",
      "Epoch 368/1000\n",
      "3488/3488 [==============================] - 2s 481us/step - loss: 0.0125 - val_loss: 0.0202\n",
      "Epoch 369/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0126 - val_loss: 0.0194\n",
      "Epoch 370/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0123 - val_loss: 0.0206\n",
      "Epoch 371/1000\n",
      "3488/3488 [==============================] - 2s 509us/step - loss: 0.0121 - val_loss: 0.0206\n",
      "Epoch 372/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0124 - val_loss: 0.0209\n",
      "Epoch 373/1000\n",
      "3488/3488 [==============================] - 1s 343us/step - loss: 0.0123 - val_loss: 0.0195\n",
      "Epoch 374/1000\n",
      "3488/3488 [==============================] - 2s 468us/step - loss: 0.0125 - val_loss: 0.0203\n",
      "Epoch 375/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0123 - val_loss: 0.0191\n",
      "Epoch 376/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0121 - val_loss: 0.0195\n",
      "Epoch 377/1000\n",
      "3488/3488 [==============================] - 1s 419us/step - loss: 0.0126 - val_loss: 0.0198\n",
      "Epoch 378/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0127 - val_loss: 0.0206\n",
      "Epoch 379/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0122 - val_loss: 0.0203\n",
      "Epoch 380/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.0120 - val_loss: 0.0193\n",
      "Epoch 381/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0128 - val_loss: 0.0220\n",
      "Epoch 382/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0124 - val_loss: 0.0203\n",
      "Epoch 383/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0124 - val_loss: 0.0202\n",
      "Epoch 384/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0125 - val_loss: 0.0185\n",
      "Epoch 385/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0126 - val_loss: 0.0205\n",
      "Epoch 386/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.0122 - val_loss: 0.0204\n",
      "Epoch 387/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0121 - val_loss: 0.0198\n",
      "Epoch 388/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0121 - val_loss: 0.0203\n",
      "Epoch 389/1000\n",
      "3488/3488 [==============================] - 2s 446us/step - loss: 0.0121 - val_loss: 0.0193\n",
      "Epoch 390/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0127 - val_loss: 0.0217\n",
      "Epoch 391/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0119 - val_loss: 0.0197\n",
      "Epoch 392/1000\n",
      "3488/3488 [==============================] - 2s 440us/step - loss: 0.0121 - val_loss: 0.0197\n",
      "Epoch 393/1000\n",
      "3488/3488 [==============================] - 2s 461us/step - loss: 0.0122 - val_loss: 0.0207\n",
      "Epoch 394/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0121 - val_loss: 0.0195\n",
      "Epoch 395/1000\n",
      "3488/3488 [==============================] - 2s 530us/step - loss: 0.0119 - val_loss: 0.0216\n",
      "Epoch 396/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0120 - val_loss: 0.0202\n",
      "Epoch 397/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0127 - val_loss: 0.0198\n",
      "Epoch 398/1000\n",
      "3488/3488 [==============================] - 2s 452us/step - loss: 0.0129 - val_loss: 0.0196\n",
      "Epoch 399/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0122 - val_loss: 0.0197\n",
      "Epoch 400/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.0122 - val_loss: 0.0194\n",
      "Epoch 401/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0119 - val_loss: 0.0204\n",
      "Epoch 402/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0119 - val_loss: 0.0193\n",
      "Epoch 403/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0120 - val_loss: 0.0219\n",
      "Epoch 404/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0120 - val_loss: 0.0186\n",
      "Epoch 405/1000\n",
      "3488/3488 [==============================] - 2s 448us/step - loss: 0.0119 - val_loss: 0.0203\n",
      "Epoch 406/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0119 - val_loss: 0.0189\n",
      "Epoch 407/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0121 - val_loss: 0.0195\n",
      "Epoch 408/1000\n",
      "3488/3488 [==============================] - 2s 531us/step - loss: 0.0119 - val_loss: 0.0205\n",
      "Epoch 409/1000\n",
      "3488/3488 [==============================] - 2s 473us/step - loss: 0.0119 - val_loss: 0.0202\n",
      "Epoch 410/1000\n",
      "3488/3488 [==============================] - 2s 530us/step - loss: 0.0118 - val_loss: 0.0209\n",
      "Epoch 411/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0118 - val_loss: 0.0192\n",
      "Epoch 412/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0116 - val_loss: 0.0213\n",
      "Epoch 413/1000\n",
      "3488/3488 [==============================] - 2s 483us/step - loss: 0.0117 - val_loss: 0.0205\n",
      "Epoch 414/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0123 - val_loss: 0.0191\n",
      "Epoch 415/1000\n",
      "3488/3488 [==============================] - 2s 534us/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 416/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.0120 - val_loss: 0.0204\n",
      "Epoch 417/1000\n",
      "3488/3488 [==============================] - 2s 466us/step - loss: 0.0122 - val_loss: 0.0193\n",
      "Epoch 418/1000\n",
      "3488/3488 [==============================] - 1s 427us/step - loss: 0.0324 - val_loss: 0.0273\n",
      "Epoch 419/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0151 - val_loss: 0.0198\n",
      "Epoch 420/1000\n",
      "3488/3488 [==============================] - 2s 533us/step - loss: 0.0137 - val_loss: 0.0194\n",
      "Epoch 421/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.0131 - val_loss: 0.0190\n",
      "Epoch 422/1000\n",
      "3488/3488 [==============================] - 2s 544us/step - loss: 0.0130 - val_loss: 0.0193\n",
      "Epoch 423/1000\n",
      "3488/3488 [==============================] - 2s 532us/step - loss: 0.0126 - val_loss: 0.0191\n",
      "Epoch 424/1000\n",
      "3488/3488 [==============================] - 2s 456us/step - loss: 0.0124 - val_loss: 0.0192\n",
      "Epoch 425/1000\n",
      "3488/3488 [==============================] - 2s 573us/step - loss: 0.0124 - val_loss: 0.0190\n",
      "Epoch 426/1000\n",
      "3488/3488 [==============================] - 2s 590us/step - loss: 0.0126 - val_loss: 0.0196\n",
      "Epoch 427/1000\n",
      "3488/3488 [==============================] - 2s 562us/step - loss: 0.0120 - val_loss: 0.0188\n",
      "Epoch 428/1000\n",
      "3488/3488 [==============================] - 2s 591us/step - loss: 0.0118 - val_loss: 0.0197\n",
      "Epoch 429/1000\n",
      "3488/3488 [==============================] - 2s 554us/step - loss: 0.0118 - val_loss: 0.0194\n",
      "Epoch 430/1000\n",
      "3488/3488 [==============================] - 2s 534us/step - loss: 0.0118 - val_loss: 0.0198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000\n",
      "3488/3488 [==============================] - 1s 279us/step - loss: 0.0117 - val_loss: 0.0190\n",
      "Epoch 432/1000\n",
      "3488/3488 [==============================] - 2s 436us/step - loss: 0.0116 - val_loss: 0.0199\n",
      "Epoch 433/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0116 - val_loss: 0.0192\n",
      "Epoch 434/1000\n",
      "3488/3488 [==============================] - 2s 488us/step - loss: 0.0117 - val_loss: 0.0200\n",
      "Epoch 435/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0115 - val_loss: 0.0192\n",
      "Epoch 436/1000\n",
      "3488/3488 [==============================] - 2s 526us/step - loss: 0.0114 - val_loss: 0.0192\n",
      "Epoch 437/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0116 - val_loss: 0.0193\n",
      "Epoch 438/1000\n",
      "3488/3488 [==============================] - 2s 495us/step - loss: 0.0117 - val_loss: 0.0200\n",
      "Epoch 439/1000\n",
      "3488/3488 [==============================] - 1s 408us/step - loss: 0.0115 - val_loss: 0.0191\n",
      "Epoch 440/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0115 - val_loss: 0.0199\n",
      "Epoch 441/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0115 - val_loss: 0.0183\n",
      "Epoch 442/1000\n",
      "3488/3488 [==============================] - 2s 510us/step - loss: 0.0118 - val_loss: 0.0188\n",
      "Epoch 443/1000\n",
      "3488/3488 [==============================] - 2s 503us/step - loss: 0.0115 - val_loss: 0.0191\n",
      "Epoch 444/1000\n",
      "3488/3488 [==============================] - 2s 499us/step - loss: 0.0113 - val_loss: 0.0198\n",
      "Epoch 445/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0114 - val_loss: 0.0189\n",
      "Epoch 446/1000\n",
      "3488/3488 [==============================] - 2s 494us/step - loss: 0.0114 - val_loss: 0.0199\n",
      "Epoch 447/1000\n",
      "3488/3488 [==============================] - 2s 443us/step - loss: 0.0115 - val_loss: 0.0186\n",
      "Epoch 448/1000\n",
      "3488/3488 [==============================] - 1s 401us/step - loss: 0.0115 - val_loss: 0.0192\n",
      "Epoch 449/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0115 - val_loss: 0.0188\n",
      "Epoch 450/1000\n",
      "3488/3488 [==============================] - 2s 461us/step - loss: 0.0112 - val_loss: 0.0195\n",
      "Epoch 451/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0113 - val_loss: 0.0189\n",
      "Epoch 452/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0112 - val_loss: 0.0197\n",
      "Epoch 453/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0112 - val_loss: 0.0189\n",
      "Epoch 454/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0114 - val_loss: 0.0195\n",
      "Epoch 455/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0114 - val_loss: 0.0184\n",
      "Epoch 456/1000\n",
      "3488/3488 [==============================] - 2s 438us/step - loss: 0.0113 - val_loss: 0.0191\n",
      "Epoch 457/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0111 - val_loss: 0.0184\n",
      "Epoch 458/1000\n",
      "3488/3488 [==============================] - 2s 512us/step - loss: 0.0112 - val_loss: 0.0213\n",
      "Epoch 459/1000\n",
      "3488/3488 [==============================] - 2s 527us/step - loss: 0.0112 - val_loss: 0.0182\n",
      "Epoch 460/1000\n",
      "3488/3488 [==============================] - 2s 492us/step - loss: 0.0115 - val_loss: 0.0224\n",
      "Epoch 461/1000\n",
      "3488/3488 [==============================] - 2s 520us/step - loss: 0.0115 - val_loss: 0.0183\n",
      "Epoch 462/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.0117 - val_loss: 0.0197\n",
      "Epoch 463/1000\n",
      "3488/3488 [==============================] - 2s 507us/step - loss: 0.0113 - val_loss: 0.0187\n",
      "Epoch 464/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0113 - val_loss: 0.0201\n",
      "Epoch 465/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0112 - val_loss: 0.0181\n",
      "Epoch 466/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0113 - val_loss: 0.0198\n",
      "Epoch 467/1000\n",
      "3488/3488 [==============================] - 2s 497us/step - loss: 0.0113 - val_loss: 0.0187\n",
      "Epoch 468/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0117 - val_loss: 0.0195\n",
      "Epoch 469/1000\n",
      "3488/3488 [==============================] - 2s 467us/step - loss: 0.0111 - val_loss: 0.0189\n",
      "Epoch 470/1000\n",
      "3488/3488 [==============================] - 1s 428us/step - loss: 0.0111 - val_loss: 0.0189\n",
      "Epoch 471/1000\n",
      "3488/3488 [==============================] - 1s 401us/step - loss: 0.0110 - val_loss: 0.0194\n",
      "Epoch 472/1000\n",
      "3488/3488 [==============================] - 1s 414us/step - loss: 0.0109 - val_loss: 0.0193\n",
      "Epoch 473/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0110 - val_loss: 0.0199\n",
      "Epoch 474/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0110 - val_loss: 0.0188\n",
      "Epoch 475/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0111 - val_loss: 0.0203\n",
      "Epoch 476/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0112 - val_loss: 0.0186\n",
      "Epoch 477/1000\n",
      "3488/3488 [==============================] - 2s 535us/step - loss: 0.0115 - val_loss: 0.0194\n",
      "Epoch 478/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0113 - val_loss: 0.0187\n",
      "Epoch 479/1000\n",
      "3488/3488 [==============================] - 2s 517us/step - loss: 0.0114 - val_loss: 0.0190\n",
      "Epoch 480/1000\n",
      "3488/3488 [==============================] - 2s 523us/step - loss: 0.0114 - val_loss: 0.0191\n",
      "Epoch 481/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0115 - val_loss: 0.0228\n",
      "Epoch 482/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.0113 - val_loss: 0.0194\n",
      "Epoch 483/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0115 - val_loss: 0.0192\n",
      "Epoch 484/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0111 - val_loss: 0.0183\n",
      "Epoch 485/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.0112 - val_loss: 0.0209\n",
      "Epoch 486/1000\n",
      "3488/3488 [==============================] - 2s 515us/step - loss: 0.0110 - val_loss: 0.0202\n",
      "Epoch 487/1000\n",
      "3488/3488 [==============================] - 2s 511us/step - loss: 0.0108 - val_loss: 0.0216\n",
      "Epoch 488/1000\n",
      "3488/3488 [==============================] - 1s 386us/step - loss: 0.0110 - val_loss: 0.0193\n",
      "Epoch 489/1000\n",
      "3488/3488 [==============================] - 1s 293us/step - loss: 0.0112 - val_loss: 0.0200\n",
      "Epoch 490/1000\n",
      "3488/3488 [==============================] - 2s 444us/step - loss: 0.0113 - val_loss: 0.0191\n",
      "Epoch 491/1000\n",
      "3488/3488 [==============================] - 2s 482us/step - loss: 0.0114 - val_loss: 0.0185\n",
      "Epoch 492/1000\n",
      "3488/3488 [==============================] - 2s 489us/step - loss: 0.0116 - val_loss: 0.0183\n",
      "Epoch 493/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0120 - val_loss: 0.0182\n",
      "Epoch 494/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0114 - val_loss: 0.0207\n",
      "Epoch 495/1000\n",
      "3488/3488 [==============================] - 1s 395us/step - loss: 0.0112 - val_loss: 0.0202\n",
      "Epoch 496/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0111 - val_loss: 0.0224\n",
      "Epoch 497/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0112 - val_loss: 0.0203\n",
      "Epoch 498/1000\n",
      "3488/3488 [==============================] - 2s 457us/step - loss: 0.0112 - val_loss: 0.0190\n",
      "Epoch 499/1000\n",
      "3488/3488 [==============================] - 2s 458us/step - loss: 0.0114 - val_loss: 0.0185\n",
      "Epoch 500/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0114 - val_loss: 0.0205\n",
      "Epoch 501/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0112 - val_loss: 0.0191\n",
      "Epoch 502/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0109 - val_loss: 0.0215\n",
      "Epoch 503/1000\n",
      "3488/3488 [==============================] - 2s 459us/step - loss: 0.0110 - val_loss: 0.0190\n",
      "Epoch 504/1000\n",
      "3488/3488 [==============================] - 2s 535us/step - loss: 0.0109 - val_loss: 0.0213\n",
      "Epoch 505/1000\n",
      "3488/3488 [==============================] - 2s 508us/step - loss: 0.0108 - val_loss: 0.0221\n",
      "Epoch 506/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0108 - val_loss: 0.0238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507/1000\n",
      "3488/3488 [==============================] - 2s 560us/step - loss: 0.0109 - val_loss: 0.0198\n",
      "Epoch 508/1000\n",
      "3488/3488 [==============================] - 2s 513us/step - loss: 0.0110 - val_loss: 0.0197\n",
      "Epoch 509/1000\n",
      "3488/3488 [==============================] - 1s 425us/step - loss: 0.0109 - val_loss: 0.0199\n",
      "Epoch 510/1000\n",
      "3488/3488 [==============================] - 1s 421us/step - loss: 0.0109 - val_loss: 0.0204\n",
      "Epoch 511/1000\n",
      "3488/3488 [==============================] - 2s 443us/step - loss: 0.0109 - val_loss: 0.0186\n",
      "Epoch 512/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0109 - val_loss: 0.0198\n",
      "Epoch 513/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0108 - val_loss: 0.0198\n",
      "Epoch 514/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0124 - val_loss: 0.0187\n",
      "Epoch 515/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0129 - val_loss: 0.0182\n",
      "Epoch 516/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0122 - val_loss: 0.0200\n",
      "Epoch 517/1000\n",
      "3488/3488 [==============================] - 1s 418us/step - loss: 0.0114 - val_loss: 0.0200\n",
      "Epoch 518/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0111 - val_loss: 0.0197\n",
      "Epoch 519/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0109 - val_loss: 0.0197\n",
      "Epoch 520/1000\n",
      "3488/3488 [==============================] - 2s 485us/step - loss: 0.0107 - val_loss: 0.0210\n",
      "Epoch 521/1000\n",
      "3488/3488 [==============================] - 2s 472us/step - loss: 0.0107 - val_loss: 0.0200\n",
      "Epoch 522/1000\n",
      "3488/3488 [==============================] - 2s 460us/step - loss: 0.0107 - val_loss: 0.0210\n",
      "Epoch 523/1000\n",
      "3488/3488 [==============================] - 2s 460us/step - loss: 0.0107 - val_loss: 0.0198\n",
      "Epoch 524/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0107 - val_loss: 0.0212\n",
      "Epoch 525/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0109 - val_loss: 0.0192\n",
      "Epoch 526/1000\n",
      "3488/3488 [==============================] - 2s 460us/step - loss: 0.0108 - val_loss: 0.0199\n",
      "Epoch 527/1000\n",
      "3488/3488 [==============================] - 2s 444us/step - loss: 0.0109 - val_loss: 0.0191\n",
      "Epoch 528/1000\n",
      "3488/3488 [==============================] - 2s 502us/step - loss: 0.0110 - val_loss: 0.0229\n",
      "Epoch 529/1000\n",
      "3488/3488 [==============================] - 2s 576us/step - loss: 0.0112 - val_loss: 0.0191\n",
      "Epoch 530/1000\n",
      "3488/3488 [==============================] - 2s 521us/step - loss: 0.0109 - val_loss: 0.0234\n",
      "Epoch 531/1000\n",
      "3488/3488 [==============================] - 2s 506us/step - loss: 0.0113 - val_loss: 0.0194\n",
      "Epoch 532/1000\n",
      "3488/3488 [==============================] - 2s 546us/step - loss: 0.0109 - val_loss: 0.0213\n",
      "Epoch 533/1000\n",
      "3488/3488 [==============================] - 2s 588us/step - loss: 0.0112 - val_loss: 0.0184\n",
      "Epoch 534/1000\n",
      "3488/3488 [==============================] - 2s 470us/step - loss: 0.0118 - val_loss: 0.0200\n",
      "Epoch 535/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0109 - val_loss: 0.0201\n",
      "Epoch 536/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0105 - val_loss: 0.0216\n",
      "Epoch 537/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0106 - val_loss: 0.0203\n",
      "Epoch 538/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0107 - val_loss: 0.0209\n",
      "Epoch 539/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0109 - val_loss: 0.0194\n",
      "Epoch 540/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0108 - val_loss: 0.0215\n",
      "Epoch 541/1000\n",
      "3488/3488 [==============================] - 2s 491us/step - loss: 0.0113 - val_loss: 0.0189\n",
      "Epoch 542/1000\n",
      "3488/3488 [==============================] - 2s 480us/step - loss: 0.0116 - val_loss: 0.0188\n",
      "Epoch 543/1000\n",
      "3488/3488 [==============================] - 2s 464us/step - loss: 0.0105 - val_loss: 0.0212\n",
      "Epoch 544/1000\n",
      "3488/3488 [==============================] - 2s 465us/step - loss: 0.0107 - val_loss: 0.0203\n",
      "Epoch 545/1000\n",
      "3488/3488 [==============================] - 2s 484us/step - loss: 0.0105 - val_loss: 0.0202\n",
      "Epoch 546/1000\n",
      "3488/3488 [==============================] - 2s 498us/step - loss: 0.0104 - val_loss: 0.0208\n",
      "Epoch 547/1000\n",
      "3488/3488 [==============================] - 2s 505us/step - loss: 0.0104 - val_loss: 0.0202\n",
      "Epoch 548/1000\n",
      "3488/3488 [==============================] - 2s 524us/step - loss: 0.0103 - val_loss: 0.0206\n",
      "Epoch 549/1000\n",
      "3488/3488 [==============================] - 2s 519us/step - loss: 0.0102 - val_loss: 0.0208\n",
      "Epoch 550/1000\n",
      "3488/3488 [==============================] - 2s 490us/step - loss: 0.0104 - val_loss: 0.0207\n",
      "Epoch 551/1000\n",
      "3488/3488 [==============================] - 2s 541us/step - loss: 0.0105 - val_loss: 0.0204\n",
      "Epoch 552/1000\n",
      "3488/3488 [==============================] - 2s 459us/step - loss: 0.0106 - val_loss: 0.0203\n",
      "Epoch 553/1000\n",
      "3488/3488 [==============================] - 2s 525us/step - loss: 0.0104 - val_loss: 0.0207\n",
      "Epoch 554/1000\n",
      "3488/3488 [==============================] - 2s 514us/step - loss: 0.0106 - val_loss: 0.0199\n",
      "Epoch 555/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0109 - val_loss: 0.0189\n",
      "Epoch 556/1000\n",
      "3488/3488 [==============================] - 2s 487us/step - loss: 0.0110 - val_loss: 0.0211\n",
      "Epoch 557/1000\n",
      "3488/3488 [==============================] - 2s 535us/step - loss: 0.0110 - val_loss: 0.0192\n",
      "Epoch 558/1000\n",
      "3488/3488 [==============================] - 2s 453us/step - loss: 0.0110 - val_loss: 0.0201\n",
      "Epoch 559/1000\n",
      "3488/3488 [==============================] - 2s 461us/step - loss: 0.0105 - val_loss: 0.0199\n",
      "Epoch 560/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 561/1000\n",
      "3488/3488 [==============================] - 2s 462us/step - loss: 0.0102 - val_loss: 0.0209\n",
      "Epoch 562/1000\n",
      "3488/3488 [==============================] - 2s 461us/step - loss: 0.0101 - val_loss: 0.0211\n",
      "Epoch 563/1000\n",
      "3488/3488 [==============================] - 2s 463us/step - loss: 0.0101 - val_loss: 0.0208\n",
      "Epoch 564/1000\n",
      "3488/3488 [==============================] - 2s 469us/step - loss: 0.0101 - val_loss: 0.0208\n",
      "Epoch 565/1000\n",
      "3488/3488 [==============================] - 2s 477us/step - loss: 0.0102 - val_loss: 0.0209\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00565: early stopping\n",
      "(414, 16, 1)\n",
      "(414, 10)\n",
      "(384, 16, 1)\n",
      "(384, 10)\n"
     ]
    }
   ],
   "source": [
    "for step in output_time_step:\n",
    "    \n",
    "    # Creating supervised training dataset\n",
    "    in_start = 0\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(df_train_scale)):\n",
    "        in_end = in_start + input_time_step\n",
    "        out_end = in_end + step\n",
    "        if out_end < len(df_train_scale):\n",
    "            x_train.append(df_train_scale[in_start:in_end, 0])\n",
    "            y_train.append(df_train_scale[in_end:out_end, 0])\n",
    "        in_start += 1\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    # Creating supervised validation dataset\n",
    "    in_start = 0\n",
    "    x_train_valid = []\n",
    "    y_train_valid = []\n",
    "    for i in range(len(df_valid_scale)):\n",
    "        in_end = in_start + input_time_step\n",
    "        out_end = in_end + step\n",
    "        if out_end < len(df_valid_scale):\n",
    "            x_train_valid.append(df_valid_scale[in_start:in_end, 0])\n",
    "            y_train_valid.append(df_valid_scale[in_end:out_end, 0])\n",
    "        in_start += 1\n",
    "    x_train_valid, y_train_valid = np.array(x_train_valid), np.array(y_train_valid)\n",
    "    x_train_valid = np.reshape(x_train_valid, (x_train_valid.shape[0], x_train_valid.shape[1], 1))\n",
    "    print(x_train_valid.shape)\n",
    "    print(y_train_valid.shape)\n",
    "    \n",
    "    # Number of samples in training set is integer multiple of the batch size. It is a limitation of the ES-LSTM\n",
    "    # implementation below\n",
    "    if (x_train.shape[0] % batch_size) != 0:\n",
    "        num = int(x_train.shape[0]/batch_size)\n",
    "        #x_train = x_train[:num]\n",
    "        #print(x_train.shape[0]-num)\n",
    "        val = batch_size * num\n",
    "        #print(val)\n",
    "        x_train = x_train[:val]\n",
    "        print(x_train.shape)\n",
    "        y_train = y_train[:val]\n",
    "        print(y_train.shape)\n",
    "    \n",
    "    \n",
    "    # Number of samples in validation set is integer multiple of batch size\n",
    "    if (x_train_valid.shape[0] % batch_size) != 0:\n",
    "        num = int(x_train_valid.shape[0]/batch_size)\n",
    "        #x_train = x_train[:num]\n",
    "        #print(x_train.shape[0]-num)\n",
    "        val = batch_size * num\n",
    "        #print(val)\n",
    "        x_train_valid = x_train_valid[:val]\n",
    "        print(x_train_valid.shape)\n",
    "        y_train_valid = y_train_valid[:val]\n",
    "        print(y_train_valid.shape)\n",
    "        \n",
    "    \n",
    "    # ES-LSTM Model\n",
    "    model_input = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    [normalized_input, denormalization_coeff] = ES(step, seasonality, batch_size, input_time_step)(model_input)\n",
    "    lstm_out = LSTM(units)(normalized_input)\n",
    "    model_output_normalized = Dense(step)(lstm_out)\n",
    "    model_output = Denormalization()([model_output_normalized, denormalization_coeff])\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    model.compile(optimizer = optimizer, loss = loss)\n",
    "    \n",
    "    \n",
    "    # Training Model\n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        validation_data = (x_train_valid, y_train_valid),\n",
    "                        callbacks = [callback], \n",
    "                        epochs = epoch, \n",
    "                        batch_size = batch_size, \n",
    "                        shuffle = False)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Preparing test dataset\n",
    "    in_start = 0\n",
    "    x_test = []\n",
    "    real = []\n",
    "    for i in range(len(df_test_scale)):\n",
    "        in_end = in_start + input_time_step\n",
    "        out_end = in_end + step\n",
    "        if out_end < len(df_test_scale):\n",
    "            x_test.append(df_test_scale[in_start:in_end, 0])\n",
    "            real.append(df_test_scale[in_end:out_end, 0])\n",
    "        in_start += 1\n",
    "    x_test, real = np.array(x_test), np.array(real)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "    print(x_test.shape)\n",
    "    print(real.shape)\n",
    "    \n",
    "    \n",
    "    # Number of samples in test set is integer multiple of the batch size.\n",
    "    if (x_test.shape[0] % batch_size) != 0:\n",
    "        num = int(x_test.shape[0]/batch_size)\n",
    "        val = batch_size * num\n",
    "        x_test = x_test[:val]\n",
    "        real = real[:val]\n",
    "        print(x_test.shape)\n",
    "        print(real.shape)\n",
    "    \n",
    "    # Prediction on Test Set\n",
    "    predictions = model.predict(x_test, batch_size=batch_size)\n",
    "    predictions = sc.inverse_transform(predictions)\n",
    "    real = sc.inverse_transform(real)\n",
    "    \n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    sum_mae = 0\n",
    "    sum_rmse = 0\n",
    "    for i in range(len(predictions)):#same as real_vector\n",
    "        mae = metrics.mean_absolute_error(real[i], predictions[i])\n",
    "        mse = metrics.mean_squared_error(real[i], predictions[i])\n",
    "        rmse = np.sqrt(mse)\n",
    "        sum_mae = sum_mae + mae\n",
    "        sum_rmse = sum_rmse + rmse\n",
    "\n",
    "    # Average MAE\n",
    "    avg_mae = sum_mae / len(predictions)\n",
    "    # Average RMSE\n",
    "    avg_rmse = sum_rmse / len(predictions)\n",
    "    \n",
    "    RMSE.append(avg_rmse)\n",
    "    MAE.append(avg_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0038677266045575075,\n",
       " 0.007339803385888919,\n",
       " 0.009700979222707741,\n",
       " 0.014525661550505152,\n",
       " 0.008495873621776185]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00355823157388431,\n",
       " 0.006949574571450555,\n",
       " 0.008936729227295288,\n",
       " 0.013633335905461511,\n",
       " 0.007563901959498724]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb386bf5f50>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deVyU9fr/8dfFAIorLrgvaK64K27tZaalaadTuZRpudTvtNdp8XQ6le3ZvktpaYvLsQ3LtEWrU7mBSwaK4g6o4AKKgsDM9fuD0S8hKipwD8z1fDx8OHzmnnveozBv7mXuj6gqxhhj/E+A0wGMMcY4wwrAGGP8lBWAMcb4KSsAY4zxU1YAxhjjpwKdDnA66tatq+Hh4U7HMMaYciU2NnaPqoYVHi9XBRAeHk5MTIzTMYwxplwRkW1FjdsuIGOM8VNWAMYY46esAIwxxk9ZARhjjJ+yAjDGGD9Vrs4CMsYUz5erkpm8MIGU9CwahYbwwIC2XN2tsdOxjI+xAjCmgvlyVTITP19LVq4bgOT0LCZ+vhbASsD8he0CMqaCmbww4dib/1FZuW4mL0xwKJHxVVYAxlQwKelZpzVu/JcVgDEVTIOalYscbxRa9LjxX1YAxlQgOXkeqga7iryvfcMa2AyApiArAGMqCFXl0S//JDHtEDf0bkrj0BCE/N/8+7aszQ/rUnn22/VWAuYYOwvImApiyi+bmR2zgzsvbcX9l7f9y30ej/JYdBxRv2wm1+3hP4MjEBGHkhpfYQVgTAWw4M+dPPftegZ3bsi9l7U57v6AAGHS0A4EuoQPfttKnlt5YkgHAgKsBPyZFYAx5dwfSencM3s13ZqF8uJ1XU74pi4i/GdwBEGuAKJ+2Uyex8PTV3eyEvBjxToGICIDRSRBRBJF5OEi7q8kIrO99y8TkXDveB0RWSwimSLy5gnWHS0if57NizDGX6WkZzF2egx1q1UialQklYOKPgB8lIgw8Yp2/OPic5i5fAcPffYHbo8dE/BXp9wCEBEX8BbQH0gCVohItKrGF1hsLLBfVVuJyHDgeWAYkA08CnT0/im87muAzLN+Fcb4ocwjedzy4Qqyc9x8Mq43YdUrFetxIsIDA9oS5ArgtR834vYok6/rgsu2BPxOcbYAegGJqrpZVXOAWcDQQssMBaZ7b88F+omIqOohVf2V/CL4CxGpBtwHPHXG6Y3xU26PctfMVWxMzeTNG7rTpn7103q8iHBv/zbc378Nn69K5p7Zq8lze0oprfFVxTkG0BjYUeDrJKD3iZZR1TwRyQDqAHtOst4ngZeAwyd7chGZAEwAaNasWTHiGlPxPfVNPIvWp/Lk1R25qM1xU70W2539WhPoCuD5BevJc3t4fUQ3glx2dri/cOR/WkS6Aueo6henWlZVo1Q1UlUjw8LO/BvdmIpixpKtfPDbVm45rwWj+jQ/6/X9v4vP4d+D2vPtn7u4/ZOV5OTZloC/KE4BJANNC3zdxDtW5DIiEgjUBPaeZJ19gUgR2Qr8CrQRkZ+KF9kY//VTQiqPR8fRr109HhnUvsTWO+6Cljx+VQTfxe/mto9jyS50MTlTMRWnAFYArUWkhYgEA8OB6ELLRAOjvbevBRbpST5uqKrvqGojVQ0Hzgc2qOrFpxveGH+SsOsgd3y6inYNavD6iG4lftB2zHkteOrqjixan8qEj6wE/MEpC0BV84A7gIXAOmCOqsaJyCQRGeJdbCpQR0QSyT+we+xUUe9v+S8DY0QkSUQiSvg1GFPhpR7M5pYPV1Al2MXUMZFUrVQ6H+G5sU9znrumE//bmMa46TFk5VgJVGRSnq4LEhkZqTExMU7HMKZMZee6GR61lIRdB5lza186NalZ6s85NzaJB+auoXeL2kwd3bPUCseUDRGJVdXIwuN2uN8YH+bxKPfPWcOapHReHd61TN78Aa7t0YRXh3Vl+ZZ9jPlgOZlH8srkeU3ZsgIwxoe9/P0Gvlm7k4cHtmNAhwZl+txDuzbm9RHdWLk9nVFTl3EgO7dMn9+UPisAY3zU3Ngk3lycyPCeTZlwYUtHMgzu3Ii3RnZjbVIGo95fRsZhK4GKxArAGB+0bPNeJn7+B+eeU4cnr+7o6KWbB3ZsyDs39iB+5wFumLqU/YdyHMtiSpYVgDE+ZsueQ9z6cSzNalfhnRt6+MQnc/tH1CdqVCQbdmcy8v1l7M084nQkUwKc/84yxhyTfjiHWz5cgQDTxvSkZpUgpyMdc0m7erx/UySb0zIZ8d5S0g5aCZR3VgDG+IicPA+3fRxL8v4som6KpHmdqk5HOs6FbcL4YExPduzLYnjUElIPHHedR1OOWAEY4wNUlUe+WMvSzft44drO9Ayv7XSkEzq3VV0+vLknOzOyGR61lF0ZVgLllRWAMT7gnZ838d/YJO7q15qruzV2Os4p9W5Zhxm39CL14BGGRS0hOT3L6UjmDFgBGOOw+Wt38sKCBIZ0acS9l7V2Ok6xRYbXZsbYXuzLzGHYlCXs2HfSK7sbH2QFYIyDVu9I597Zq+neLJQXru3s6OmeZ6J7s1p8Mr43B7JyGTZlCdv2HnI6kjkNVgDGOCQ5PYtx02MIq16JqJtOPZ+vr+rcJJRPx/chK9fNsClL2Zxms7yWF1YAxjjgYHYuYz9cwZFcNx+M6UndasWbz9dXdWxck0/H9yHH7WFY1FISUw86HckUgxWAMWUsz+3hTu98vm/f2J3Wpzmfr69q37AGsyb0QZVjVy81vs0KwJgy9tQ36/gpIY0nh3bkgtYVa5rTNvWrM2tCHwJEGPHeUuJTDjgdyZyEFYAxZWj671v58PetjDu/BSN7N3M6TqloVa8as2/tS6XAAEa+v5Q/kzOcjmROwArAmDKyeH0qT8yL47L29Zl4ZcnN5+uLWtStyuwJfakaHMjI95ayZke605FMEawAjCkD63Ye4I5PV9K+YQ1eG961xOfz9UXN6lRh1oQ+1KwSxI3vLyN2236nI5lCrACMKWWpB7MZ++EKqlUO9LvpFZvWrsLsCX2pUy2Ym6YuY8XWfU5HMgVYARhTirJy3IyfEcv+w7lMHd2TBjUrOx2pzDUKDWHWhL7Ur1GZ0dOWs2TTXqcjGS8rAGNKicej3P/f1fyRlM5rw7vSsXHZzOfrixrUrMysW/vQKDSEmz9czq8b9zgdyWAFYEypefG7BOav3cW/rmjP5WU8n68vqle9MrMm9CG8TlXGTl/BzxvSnI7k96wAjCkFc2J28PZPmxjRqxnjLmjhdByfUbdaJT4d34dzwqoxfnoMP67b7XQkv2YFYEwJW7JpL498sZbzW9Vl0tAO5e4Cb6WtdtVgPh3fm7YNqnPbx7EsjNvldCS/ZQVgTAnanJbJbR/H0rxOVd66obtPzOfri0KrBPPxuN5ENKrJ7Z+sZP7anU5H8kv23WlMCdl/KH8+X1eAMG10T2qG+M58vr6oZkgQH43tRZemodw5cxXz1qQ4HcnvWAEYUwJy8jzc+nEsKRnZvHdTD5rVqeJ0pHKhRuUgpt/Six7NanH3rFV8sSrJ6Uh+xQrAmLOkqkz8fC3Lt+xj8rWd6dHcd+fz9UXVKgXy4S096d2iDvfNWcOcmB1OR/IbxSoAERkoIgkikigiDxdxfyURme29f5mIhHvH64jIYhHJFJE3CyxfRUS+EZH1IhInIs+V1Asypqy9/dMmPluZxD2XtWZoV9+fz9cXVQkOZNqYnpzfqi4Pzv2Dmcu3Ox3JL5yyAETEBbwFXAFEACNEJKLQYmOB/araCngFeN47ng08CvyziFW/qKrtgG7AeSJyxZm9BGOc880fO5m8MIGhXRtxd7/yM5+vLwoJdvHeTZFc3DaMiZ+v5aMlW52OVOEVZwugF5CoqptVNQeYBQwttMxQYLr39lygn4iIqh5S1V/JL4JjVPWwqi723s4BVgJNzuJ1GFPmVm3fz31zVhPZvBbP/738zefriyoHuZgyqgeXta/Ho1/FMe3XLU5HqtCKUwCNgYI75ZK8Y0Uuo6p5QAZQpzgBRCQUuAr48QT3TxCRGBGJSUuzTw4a35C0/zDjZ8RQv0ZlpozqUW7n8/VFlQJdvH1DDwZ0qM+kr+OJ+mWT05EqLEcPAotIIDATeF1VNxe1jKpGqWqkqkaGhVWs2ZNM+ZQ/n28MR/I8TBsTSZ1yPp+vLwoODODNkd0Z1Kkhz8xfz1uLE52OVCEV57q0yUDTAl838Y4VtUyS9029JlCcS/5FARtV9dViLGuM4/LcHu74dBWb0jKZfksvWtWrGPP5+qIgVwCvDe9KoEuYvDCBPLdy92V2nKUkFacAVgCtRaQF+W/0w4GRhZaJBkYDS4BrgUWqqidbqYg8RX5RjDvd0MY4QVV5Yl48P29I49lrOnFeq7pOR6rwAl0BvHx9/gQ6r/ywgTyPh/v6t7HjLSXklAWgqnkicgewEHAB01Q1TkQmATGqGg1MBT4SkURgH/klAYCIbAVqAMEicjVwOXAAeARYD6z0/me+qarvl+SLM6Ykffj7Vj5auo0JF7ZkRK+KOZ+vL3IFCJOv7UJggPDGokRy3cpDA9taCZSAYk1NpKrzgfmFxv5T4HY2cN0JHht+gtXa/54pNxat382TX8dzeUR9HhrYzuk4fscVIDx3TWcCXQG8+/Mm8tweHhnU3krgLPnP3HTGnKH4lAPc+ekqIhrV4FU/mc/XFwUECE9f3ZGgAOH9X7eQ51EeuyrCSuAsWAEYcxKpB7IZO30F1SsHMXV0T6oE24+Mk0SEx4d0INAVwNRft5Dr9vDk0I4EWCmfEftuNuYEsnLcjJsRQ0ZWLnNuzZ/T1jhPRPj3oPYEuoQpP2/G7VGe+VsnK4EzYAVgTBE8HuXe2atZm5zBe6Mi/Xo+X18kIjw8sB1BAQG8uTj/wPAL13a23XOnyQrAmCK8sDCBBXG7+Peg9lwWUd/pOKYIIsI/B7QlyBVw7BTRl67rQqBNwlNsVgDGFDJnxQ7e/XkTN/RuxtjzbT5fX3f3Za3/78NiHuXVYV1tJrZisgIwpoDfN+3hX1+s5YLWdXl8iM3nW17cfkkrAgOEZ79dj9utvD6iG8GBVgKnYv9CxnhtSsvkto9iaVHX5vMtj2696BweHRzBgrhd/OOTlRzJczsdyefZd7gxwD7vfL5BrgCmjelJjco2n295NPb8Fkwa2oEf1u3mto9iyc61EjgZKwDj947kubnto1h2ZmQTdVMkTWvbfL7l2U19w3nmb51YnJDG+BkxVgInYQVg/JqqMvGztSzfuo8Xr+tCj+a1nI5kSsDI3s144e+d+TVxD7d8uILDOXlOR/JJVgDGr725KJHPVyVzX/82DOnSyOk4pgRd37MpL13XhaWb9zLmgxVkHrESKMwKwPiteWtSeOn7DfytW2PuvLSV03FMKbimexNeGdaV2G37GT1tOQezc52O5FOsAIxfit22n/v/u4ae4bV47u+d7HTPCmxo18a8Prwba3akM2rqcjKyrASOsgIwfmfHvsNMmBFDw5qVmTIqkkqBNp9vRTeoc0PeHNmduJQMRk1dRvrhHKcj+QQrAONXDmTnMnb6CnLdHqaO7kntqsFORzJlZGDHBrx7Yw/W7zzIyPeWse+QlYAVgPEbeW4Pt3+yks1ph3j3xh60qlfN6UimjPVrX5+om3qQmJbJyPeWsifziNORHGUFYPyCqvL4vDj+t3EPT/+tI+fafL5+6+K29Zg6OpItew4xImopqQeznY7kGCsA4xc++G0rHy/dzq0XtWRYT5vP199d0DqMD27uSdL+LIZHLWX3Af8sASsAU+H9EL+bJ7+JZ2CHBjw0wObzNfnOPacu02/pxe6MbIZNWUJKepbTkcqcFYCp0OJSMrhr1io6NqrJK8O62qxR5i96tajNjLG92JOZw7CoJSTtP+x0pDJlBWAqrN0Hshk3PYaaIUG8PzqSkGA73dMcr0fz2nw8rjfph3MZNmUp2/f6TwlYAZgK6XBOHuOmx3AgK5epo3vafL7mpLo2DWXm+D5kHsljWNQStu455HSkMmEFYCocj0e5Z9Zq4lIyeGNkNyIa1XA6kikHOjauyczxfcjOdTMsagmb0jKdjlTqrABMhfP8gvV8F7+bfw+K4NJ2Np+vKb6IRjWYOaEPeW5l2JSlbNx90OlIpcoKwFQos5ZvZ8ovmxnVpzk3nxfudBxTDrVrUINZE/ogAsOjlrJ+1wGnI5UaKwBTYfyWuId/f/knF7UJ47GrIuwCb+aMta5fnVkT+hDoEkZELSUuJcPpSKXCCsBUCImpmdz2cSwtw6ryxshuBNp8vuYsnRNWjdkT+hIS5GLke8tYm1TxSqBYPyUiMlBEEkQkUUQeLuL+SiIy23v/MhEJ947XEZHFIpIpIm8WekwPEVnrfczrYr+umTN0dD7fSoEBTB1t8/makhNetyqzb+1LtUqBjHx/Kau273c6Uok6ZQGIiAt4C7gCiABGiEhEocXGAvtVtRXwCvC8dzwbeBT4ZxGrfgcYD7T2/hl4Ji/A+LcjeW4mzIhh9wGbz9eUjqa1qzD71j7UqhLMqKnLid22z+lIJaY4WwC9gERV3ayqOcAsYGihZYYC07235wL9RERU9ZCq/kp+ERwjIg2BGqq6VFUVmAFcfTYvxPgfVeXhz9YSs20/L13fhe7NbD5fUzqa1MovgbDqlbhp6nKWbd7rdKQSUZwCaAzsKPB1knesyGVUNQ/IAOqcYp1Jp1gnACIyQURiRCQmLS2tGHGNv3hjUSJfrErmn5e3YXBnm8/XlK6GNUOYNaEPDWpWZswHK/g9cY/Tkc6azx8pU9UoVY1U1ciwsDCn4xgf8dXqZF7+fgPXdG/M7ZfYfL6mbNSvUZlZE/rStHYIN3+4gv9tLN+/lBanAJKBpgW+buIdK3IZEQkEagIn20ZK9q7nZOs0pkix2/bxwNw/6BVem2evsfl8TdkKq16JmeP70KJuVcZOj2FxQqrTkc5YcQpgBdBaRFqISDAwHIgutEw0MNp7+1pgkXfffpFUdSdwQET6eM/+uQn46rTTG7/x5apkzntuES0e/obr3l1CjUoupozqYfP5GkfUqZZfAq3rVePWGbH8EL/b6Uhn5JQF4N2nfwewEFgHzFHVOBGZJCJDvItNBeqISCJwH3DsVFER2Qq8DIwRkaQCZxD9A3gfSAQ2Ad+WzEsyFc2Xq5KZ+PlaktOzUMCjcPCIm583lO/Nb1O+1aoazKfj+tCuYXVu+ziWBX/ucjrSaZOT/KLucyIjIzUmJsbpGKaMnffcIpKLmKyjcWgIvz18qQOJjPk/B7JzGT1tOX8kZfD68G4M6tzQ6UjHEZFYVY0sPO7zB4GNf9ux73CRb/6AX87gZHxPjcpBzLilF92ahnLnzJV8tbr8HM60AjA+KfNIHi8sWE+/l3/mRId4G4WGlGkmY06keuUgpt/Si57htbl39mo+i0069YN8gBWA8SkejzI3NolLX/yJt3/axKBODXlsSAQhQX892BsS5OKBAW0dSmnM8apWCuSDm3vSp2Ud/jl3DXNW7Dj1gxwW6HQAY46K3baPSfPiWZOUQZemobw7qsexT/eGhgQzeWECKelZNAoN4YEBbbm6W5GfHTTGMVWCA5k2pifjZ8Tw4Gd/kOvxcEPv5k7HOiE7CGwcl5KexfML1vPV6hTqVa/EQwPb8bdujW0Cd1NuZee6+ccnK1m0PpUnhnRg9LnhjuY50UFg2wIwjsnKcRP1y2be+TkRj8Idl7Ti/118DlUr2belKd8qB7l458bu3PHpKh6LjiPX7WHcBS2djnUc+0kzZU5VmffHTp6bv46UjGyu7NSAiVe0tyt5mgqlUqCLt0Z2566Zq3jqm3XkeZTbLjrH6Vh/YQVgytTapAyemBdHzLb9RDSswcvDutKn5cmuG2hM+RUcGMAbI7tx7+zVPPftevLcHu64tLXTsY6xAjBlIvVgNpMXJDB3ZRK1qwTz7DWduD6yKS7bz28quCBXAK8O60pggPDidxvIdSv3XNbaJ65hZQVgStWRPDfTft3Km4s2kuP2MP6CltxxaSubtcv4lUBXAC9d35VAVwCv/biRPI+Hf17e1vESsAIwpUJV+S5+N09/s47t+w5zWft6PDIoghZ1qzodzRhHuAKEF/7emcAA4a3Fm8hzKw9f0c7RErACMCVu/a4DTJoXz++b9tK6XjVm3NKLC9vYXA7GBAQIz/ytE4EuYcovm8l1K48Obu9YCVgBmBKz71AOL3+fwKfLtlO9chBPDOnADb2bEeiyD5wbc1RAgPDk0I4EBgQw7bct5Hk8PH5VB0c+92IFYM5artvDjCXbeO2HDRzKcTOqT3PuuawNtaoGOx3NGJ8kIjx2VQRBLuG9/20h1608fXXHMi8BKwBzVhYnpPLU1/FsSjvEBa3r8ujgCNrUr+50LGN8nojwryvbE+gK4J2fNuH2eHj2ms5lemacFYA5I4mpmTz1TTw/JaQRXqcKU0dHcmm7eo6f1WBMeSIiPDigLUGuAF7/cSN5bmXydV3KrASsAMxpyTicy2s/bmTGkq2EBLl45Mr2jD43nOBA289vzJkQEe7r34bAAOHl7zeQ61Feub5LmRw7swIwxZLn9jBrxQ5e+i6B9Kxchvdsyv2Xt6VutUpORzOmQrirX2sCXcILCxJwezy8NrwbQaVcAlYA5pR+T9zDpK/jWb/rIL1a1OY/gyPo2Lim07GMqXD+cXErggICeHr+Otyelbwxonupbl1bAZgT2r73ME/Pj2dh3G6a1Arh7Ru6c0XHBraf35hSNP7ClgS6hCfmxXP1W7+SfjiXnRnZpTIPhhWAOU7mkTzeWpzI1P9tIdAlPDCgLWPPb0HlQrNyGWNKx83ntSAu5QBzC0wtmZyexcTP1wKUWAlYAZhjPB5l7sokJi9MIO3gEa7p3piHBrajfo3KTkczxu8s2bT3uLGsXDeTFyZYAZiSFbN1H0/Mi2dtcgbdmoXy3k2RdG0a6nQsY/xWSnrWaY2fCSsAP5ecnsVz365n3poUGtSozKvDujKkSyObjtEYhzUKDSG5iDf7RqEhJfYcVgB+KivHzbs/b2LKL5tQhbsubcVtF59DlWD7ljDGFzwwoC0TP19LVq772FhIkIsHBrQtseewn3Y/o6pEr0nhuW/XszMjm8GdG/LwFe1oUsumYzTGlxzdzz95YQIp6Vl2FpA5O2t2pDPp63hit+2nY+MavDa8G71a1HY6ljHmBK7u1rhE3/ALswLwA6kHsnlhYQJzY5OoWy2YF/7emb/3aGLTMRrj54r1ETMRGSgiCSKSKCIPF3F/JRGZ7b1/mYiEF7hvonc8QUQGFBi/V0TiRORPEZkpInauYQnLznXz1uJELnnxJ6JXp3DrRS1Z/M+Lub6nzcVrjCnGFoCIuIC3gP5AErBCRKJVNb7AYmOB/araSkSGA88Dw0QkAhgOdAAaAT+ISBugAXAXEKGqWSIyx7vchyX30vyXqrIwbhdPz1/Hjn1Z9I+ozyNXtifcpmM0xhRQnF1AvYBEVd0MICKzgKFAwQIYCjzuvT0XeFPyrxcwFJilqkeALSKS6F3fdu9zh4hILlAFSDn7l2PiUw4w6es4lm7eR9v61flkXG/Oa1XX6VjGGB9UnAJoDOwo8HUS0PtEy6hqnohkAHW840sLPbaxqi4RkRfJL4Is4DtV/a6oJxeRCcAEgGbNmhUjrn/am3mEl77fwKzl26kZEsSTQzswopdNx2iMOTFHDgKLSC3ytw5aAOnAf0XkRlX9uPCyqhoFRAFERkZqmQYtB3LyPMxYspXXftzI4Rw3o88N555+bahZJcjpaMYYH1ecAkgGmhb4uol3rKhlkkQkEKgJ7D3JYy8DtqhqGoCIfA6cCxxXAObEFq9P5cmv49m85xAXtQnj0cHtaVXPpmM0xhRPcQpgBdBaRFqQ/+Y9HBhZaJloYDSwBLgWWKSqKiLRwKci8jL5B4FbA8sBD9BHRKqQvwuoHxBTAq/HLySmHuTJr9fx84Y0WoZV5YMxPbmkXT2nYxljyplTFoB3n/4dwELABUxT1TgRmQTEqGo0MBX4yHuQdx/5JYF3uTnkHzDOA25XVTewTETmAiu946vw7uYxJ5ZxOJdXf9zAjCXbqBLs4t+D2nNTX5uO0RhzZkS1/OxWj4yM1JgY/9tQyHN7mLl8Oy9/v4GMrFyG92rG/f3bUMemYzTGFIOIxKpqZOFx+ySwj/stcQ+T5sWTsPsgfVvW4T9XRdC+YQ2nYxljKgArAB+1dc8hnp6/ju/jd9O0dgjv3tidAR1sOkZjTMmxAvAxB7NzeXNxIh/8upUgl/DgwLbccp5Nx2iMKXlWAD7C41HmxibxwsIE9mQe4doeTXhwQFvq2XSMxphSYgXgkC9XJR+7znedasFUDgwgKT2bHs1rMW1MJJ2b2HSMxpjSZQXggC9XJf9lpp89mTkAjOrbnElDOth+fmNMmbATyB0weWHCX6Z5O2rRulR78zfGlBkrAAekFDHR88nGjTGmNFgBOKBejaI/wNUoNKSMkxhj/JkVgAPqVz++AEKCXDwwoK0DaYwx/soKoIz9unEPfyQf4MqODWgcGoIAjUNDePaaTqU6+bMxxhRmZwGVoVy3h8fnxdGsdhVeHtbVPtxljHGUbQGUoem/byUxNZP/DI6wN39jjOOsAMpI6sFsXv1hIxe3DaNfe7t2vzHGeVYAZeS5b9eTk+fhsavsg17GGN9gBVAGYrft4/OVyYy9oAUt6lZ1Oo4xxgBWAKXO7VEei46jQY3K3HFJK6fjGGPMMVYApWz2ih38mXyAiVe2o2olO+nKGOM7rABKUfrhHCYvXE+vFrUZ0qWR03GMMeYvrABK0Uvf5c/h+4Rd4dMY44OsAEpJfMoBPlm2jVF9mtscvsYYn2QFUApUlcei/yS0SjD39bfr+xhjfJMVQCn4anUKK7bu58EBbalZJcjpOMYYUyQrgBKWeSSPZ+avo3OTmlwf2dTpOMYYc0J2XmIJe2PRRlIPHmHKqB4EBNiBX2OM77ItgBK0KS2Tab9u4doeTejWrJbTcYwx5qSsAEqIqvLEvHgqB7p4aGA7p+MYY8wpWQGUkO/jd/PLhjTu6d+GsCJm/DLGGF9TrAIQkYEikiAiiSLycJcL0BcAAAvSSURBVBH3VxKR2d77l4lIeIH7JnrHE0RkQIHxUBGZKyLrRWSdiPQtiRfkhOxcN09+E0/retW4qW9zp+MYY0yxnLIARMQFvAVcAUQAI0QkotBiY4H9qtoKeAV43vvYCGA40AEYCLztXR/Aa8ACVW0HdAHWnf3LcUbUL5vZsS+LJ4Z0IMhlG1XGmPKhOO9WvYBEVd2sqjnALGBooWWGAtO9t+cC/ST/2gdDgVmqekRVtwCJQC8RqQlcCEwFUNUcVU0/+5dT9pL2H+atxYlc2akB57aq63QcY4wptuIUQGNgR4Gvk7xjRS6jqnlABlDnJI9tAaQBH4jIKhF5X0SKvFC+iEwQkRgRiUlLSytG3LL19DfrEIFHBhXeKDLGGN/m1P6KQKA78I6qdgMOAccdWwBQ1ShVjVTVyLCwsLLMeEq/Je7h2z93cfvFrWgcGuJ0HGOMOS3FKYBkoOBHWpt4x4pcRkQCgZrA3pM8NglIUtVl3vG55BdCuZHr9vBYdBzNaldh/IUtnY5jjDGnrTgFsAJoLSItRCSY/IO60YWWiQZGe29fCyxSVfWOD/eeJdQCaA0sV9VdwA4ROXqltH5A/Fm+ljI1/fetJKZm8ujgCCoHuU79AGOM8TGnvBSEquaJyB3AQsAFTFPVOBGZBMSoajT5B3M/EpFEYB/5JYF3uTnkv7nnAberqtu76juBT7ylshm4uYRfW6lJPZjNqz9s5OK2YVzWvp7TcYwx5oxI/i/q5UNkZKTGxMQ4HYP756whek0yC++5kJZh1ZyOY4wxJyUisaoaWXjcTlo/TbHb9vPZyiTGnt/S3vyNMeWaFcBpcHuUx6PjqF+jEnde2srpOMYYc1asAE7D7BU7WJucwb+ubE/VSnYlbWNM+WYFUEzph3OYvHA9vcJrM6RLI6fjGGPMWbMCKKaXv99ARlYujw/pQP5VLowxpnyzAiiG+JQDfLx0Gzf2aU5EoxpOxzHGmBJhBXAKqvkHfmuGBHFf/zZOxzHGmBJjBXAK0WtSWL51Hw8ObEdolWCn4xhjTImxAjiJQ0fyeGb+Ojo1rsn1kU1P/QBjjClH7FzGk3hjUSK7DxzhnRt74AqwA7/GmIrFtgBOYFNaJlN/3czfuzehe7NaTscxxpgSZwVQBFVl0rx4KgW6eOiKtqd+gDHGlENWAEX4YV0qP29I457LWlOvemWn4xhjTKmwAigkO9fNpK/jaF2vGqPPDXc6jjHGlBo7CFxI1C+b2bEvi0/G9SbIZf1ojKm47B2ugKT9h3n7p0Su6NiA81rVdTqOMcaUKiuAAp6Zvw6ARwa1dziJMcaUPisAr98S9zB/7S7+cXErmtSq4nQcY4wpdVYAQK7bw2PRcTStHcKEC1s6HccYY8qEFQAw/fetJKZm8uigCCoHuZyOY4wxZcLvCyDt4BFe+2EjF7UJo39EfafjGGNMmfH7Anh+wXqy89w8dlWETfRijPErfl0AK7fvZ25sErec34KWYdWcjmOMMWXKbwvA7VEe+yqOetUrceelrZ2OY4wxZc5vC2BOzA7WJmfwryvbU62SfSDaGON//LIAMg7nMnlhAj3DazG0ayOn4xhjjCP8sgBe/j6B9MM5PDGkox34Ncb4Lb8rgPiUA3y0dBs39G5ORKMaTscxxhjHFKsARGSgiCSISKKIPFzE/ZVEZLb3/mUiEl7gvone8QQRGVDocS4RWSUiX5/tCykOVeXx6DhqhgRx/+VtyuIpjTHGZ52yAETEBbwFXAFEACNEJKLQYmOB/araCngFeN772AhgONABGAi87V3fUXcD6872RRRX9JoUlm/dxwMD2hFaJbisntYYY3xScbYAegGJqrpZVXOAWcDQQssMBaZ7b88F+kn+zvWhwCxVPaKqW4BE7/oQkSbAIOD9s38Zp3boSB7PzF9Hx8Y1GNazaVk8pTHG+LTinP/YGNhR4OskoPeJllHVPBHJAOp4x5cWemxj7+1XgQeB6id7chGZAEwAaNasWTHi/tWXq5KZvDCB5PQsAIb3bIYrwA78GmOMIweBRWQwkKqqsadaVlWjVDVSVSPDwsJO63m+XJXMxM/XHnvzh/wZv75clXzamY0xpqIpTgEkAwX3mTTxjhW5jIgEAjWBvSd57HnAEBHZSv4upUtF5OMzyH9SkxcmkJXr/stYVq6byQsTSvqpjDGm3ClOAawAWotICxEJJv+gbnShZaKB0d7b1wKLVFW948O9Zwm1AFoDy1V1oqo2UdVw7/oWqeqNJfB6/iKlwG/+xRk3xhh/cspjAN59+ncACwEXME1V40RkEhCjqtHAVOAjEUkE9pH/po53uTlAPJAH3K6q7iKfqBQ0Cg35y+6fguPGGOPvJP8X9fIhMjJSY2Jiir380WMABXcDhQS5ePaaTlzdrfFJHmmMMRWHiMSqamTh8Qp9FbSjb/KTFyaQkp5Fo9AQHhjQ1t78jTGGCl4AkF8C9oZvjDHH87trARljjMlnBWCMMX7KCsAYY/yUFYAxxvgpKwBjjPFT5epzACKSBmw7w4fXBfaUYJySYrlOj+U6PZbr9FTUXM1V9biLqZWrAjgbIhJT1AchnGa5To/lOj2W6/T4Wy7bBWSMMX7KCsAYY/yUPxVAlNMBTsBynR7LdXos1+nxq1x+cwzAGGPMX/nTFoAxxpgCrACMMcZPVegCEJGmIrJYROJFJE5E7nY601EiUllElovIGm+2J5zOdJSIuERklYh87XSWgkRkq4isFZHVIlL8iSFKmYiEishcEVkvIutEpK8PZGrr/Xc6+ueAiNzjdC4AEbnX+z3/p4jMFJHKTmcCEJG7vZninPy3EpFpIpIqIn8WGKstIt+LyEbv37VK4rkqdAGQPwvZ/aoaAfQBbheRCIczHXUEuFRVuwBdgYEi0sfhTEfdDaxzOsQJXKKqXX3sXO3XgAWq2g7ogg/826lqgvffqSvQAzgMfOFwLESkMXAXEKmqHcmfZXC4s6lARDoC44Fe5P8fDhaRVg7F+RAYWGjsYeBHVW0N/Oj9+qxV6AJQ1Z2qutJ7+yD5P5g+MTmA5sv0fhnk/eP4EXkRaQIMAt53Okt5ICI1gQvJnxYVVc1R1XRnUx2nH7BJVc/0U/QlLRAIEZFAoAqQ4nAegPbAMlU9rKp5wM/ANU4EUdVfyJ9at6ChwHTv7enA1SXxXBW6AAoSkXCgG7DM2ST/x7urZTWQCnyvqr6Q7VXgQcDjdJAiKPCdiMSKyASnw3i1ANKAD7y7zd4XkapOhypkODDT6RAAqpoMvAhsB3YCGar6nbOpAPgTuEBE6ohIFeBKoKnDmQqqr6o7vbd3AfVLYqV+UQAiUg34DLhHVQ84necoVXV7N9GbAL28m6GOEZHBQKqqxjqZ4yTOV9XuwBXk78670OlA5P822x14R1W7AYcooc3zkiAiwcAQ4L9OZwHw7rseSn5xNgKqisiNzqYCVV0HPA98BywAVgPukz7IIZp/7n6J7C2o8AUgIkHkv/l/oqqfO52nKN5dBos5fr9fWTsPGCIiW4FZwKUi8rGzkf6P97dHVDWV/P3ZvZxNBEASkFRg620u+YXgK64AVqrqbqeDeF0GbFHVNFXNBT4HznU4EwCqOlVVe6jqhcB+YIPTmQrYLSINAbx/p5bESit0AYiIkL9vdp2qvux0noJEJExEQr23Q4D+wHonM6nqRFVtoqrh5O82WKSqjv92BiAiVUWk+tHbwOXkb7Y7SlV3ATtEpK13qB8Q72CkwkbgI7t/vLYDfUSkivfnsx8+cNAcQETqef9uRv7+/0+dTfQX0cBo7+3RwFclsdKKPin8ecAoYK13XzvAv1R1voOZjmoITBcRF/lFPEdVfeq0Sx9TH/gi/z2DQOBTVV3gbKRj7gQ+8e5u2Qzc7HAe4FhR9gdudTrLUaq6TETmAivJP0tvFb5z+YXPRKQOkAvc7tTBfBGZCVwM1BWRJOAx4DlgjoiMJf+S+NeXyHPZpSCMMcY/VehdQMYYY07MCsAYY/yUFYAxxvgpKwBjjPFTVgDGGOOnrACMMcZPWQEYY4yf+v8iV1gKM05J8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(output_time_step,RMSE, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb39267fe90>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(output_time_step,MAE, marker='o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
